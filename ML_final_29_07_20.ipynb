{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_final_29_07_20.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1S_XkkvqF1_Si4h3yGLHgmpIhOMXFh5A7",
      "authorship_tag": "ABX9TyPM8fSyv96f0dDTasjR4NZO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayantikag98/Internship/blob/ML_INTERN/ML_final_29_07_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsgKD_kEgUqT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62b497c2-3409-4598-ac4e-cbe6a74b7600"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbR3Yrm6gmKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#! pip install adtk\n",
        "from adtk.data import validate_series\n",
        "from adtk.visualization import plot\n",
        "from adtk.detector import SeasonalAD\n",
        "from adtk.data import to_events\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.mplot3d import Axes3D  \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os \n",
        "import tempfile\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "import random\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import DBSCAN\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6swwaDFVyhmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "2ee719ad-f538-4ad3-dbf6-bb6f0d578dde"
      },
      "source": [
        "### Approx 100 machines and 1 day data\n",
        "\n",
        "    \n",
        "class AnomalyDetection:\n",
        "\n",
        "  ### INITIATOR ###\n",
        "\n",
        "  def __init__(self):\n",
        "    # ! pip install pymongo\n",
        "    # try:\n",
        "      # import pymongo\n",
        "      # from pymongo import MongoClient\n",
        "      # import json\n",
        "    # except Exception as e:\n",
        "    #   print(\"Some modules are missing\")\n",
        "    pass\n",
        "    \n",
        "\n",
        "\n",
        " ### LOAD DATA ###\n",
        "\n",
        "  def loadData (self):\n",
        "    copied_path=\"drive/My Drive/Data_final_version.csv\"\n",
        "    data=pd.read_csv(copied_path,parse_dates=[\"part_cycle_time\"])\n",
        "    #print(data.dtypes)\n",
        "    return data\n",
        "\n",
        "    ### CODE TO IMPORT DATA STORED IN MONGODB\n",
        "\n",
        "  #def importData(self):\n",
        "    ## To import the data from MongoDB server as pandas DataFrame\n",
        "    # client = MongoClient()\n",
        "    # db = client.DataIntern\n",
        "    # collection = db.AnomalyDetect\n",
        "    # data = pd.DataFrame(list(collection.find()))\n",
        "    # return data\n",
        "\n",
        "### TO MAKE SOME CHANGES IN SOME PARTICULAR COLUMNS ###\n",
        "\n",
        "  # def columnManipulation(self,df):\n",
        "  #   ## To correct the values of certain columns\n",
        "  #   import numpy as np\n",
        "  #   import itertools\n",
        "  #   np.random.seed(1)\n",
        "  #   totalItem=1000\n",
        "  #   randomNum1=np.random.randint(60,1800,totalItem)\n",
        "  #   randomList=list(randomNum1)\n",
        "  #   # to generate 1000 evenly spaced numbers between 1 and 1000\n",
        "  #   ranList1=np.linspace(1,totalItem,totalItem,dtype=\"int\")\n",
        "  #   ranList2=np.random.randint(1,200,totalItem)\n",
        "  #   #print(ranList2)\n",
        "  #   # to make the part_count and part_ID column \n",
        "  #   lis=[]\n",
        "  #   lis1=[]\n",
        "  #   for i in range(len(randomList)):\n",
        "  #       lis.append(np.repeat(ranList1[i],randomList[i]))\n",
        "  #       lis1.append(np.repeat(ranList2[i],randomList[i]))\n",
        "  #   df['part_count']=list(itertools.chain.from_iterable(lis))\n",
        "  #   df['part_ID']=list(itertools.chain.from_iterable(lis1))  \n",
        "  #   return df\n",
        "\n",
        "  \n",
        "  ### TO IMPUTE USING KNN IMPUTER BUT DID NOT USE ###\n",
        "\n",
        "  # def knnImpute(self,df,column):\n",
        "  #   import numpy as np\n",
        "  #   from sklearn.impute import KNNImputer\n",
        "  #   x=df[column].values\n",
        "  #   imputer= KNNImputer(n_neighbors=2)\n",
        "  #   print(\"\\n\")\n",
        "  #   print(\"The result after KNN Imputation\")\n",
        "  #   print(imputer.fit_transform(x.reshape(-1,1)))\n",
        "  #   print(\"\\n\")\n",
        "\n",
        "\n",
        "### FOR MAKING SOME CHANGES IN SOME OF THE COLUMNS IN THE DATA ###\n",
        "\n",
        "  def dataManipulation(self):\n",
        "    ## To remove the unneccessary columns and to store a copy of the dataframe\n",
        "    ## To create only time and time_in_second column\n",
        "    #print(data['_id'].nunique())\n",
        "    data=self.loadData()\n",
        "    df1=data\n",
        "    df=data\n",
        "    df['part_cycle_time']=df1['part_cycle_time'].dt.time\n",
        "    data1=self.loadData()\n",
        "    data2=self.loadData()\n",
        "    data3=self.loadData()\n",
        " \n",
        "    d=((data1['part_cycle_time'].dt.minute)*60)+data2['part_cycle_time'].dt.second\n",
        "    df.insert(1, 'time_in_second', d)\n",
        "    df.insert(2,'cumsum_in_minute',((df['time_in_second'].cumsum())/60).astype(int))\n",
        "    df.insert(3,'cumsum_in_hour',((df['time_in_second'].cumsum())/3600).astype(int))\n",
        "    df.insert(1,'time_in_minute',data2['part_cycle_time'].dt.minute)\n",
        "    df.insert(0,'part_cycle_time_with_date',data3['part_cycle_time'])\n",
        "    df['material'].replace({'MILD_STEEL':1},inplace=True)\n",
        "    df['quality1']=df['quality']\n",
        "    df.drop(\"quality\",axis=1,inplace=True)\n",
        "    df.rename(columns={\"quality1\":\"quality\"},inplace=True)\n",
        "    df['running_tool_number'].fillna(0, inplace=True)\n",
        "    df['tool_changed'].fillna(0,inplace=True)\n",
        "    #self.knnImpute(df,'tolerance')\n",
        "    df['tolerance'].fillna(df['tolerance'].mean(),inplace=True) \n",
        "    df['cutting_speed'].fillna(df['cutting_speed'].mean(),inplace=True)\n",
        "    df['cutting_speed'].fillna(df['cutting_speed'].median(),inplace=True)\n",
        "    # for i in range(len(l1)):\n",
        "    #   comparison=l1[i]==l2[i]\n",
        "    #   if (comparison == False):\n",
        "    #     print(f\"{l1[i]} and {l2[i]}\")\n",
        "    #     break\n",
        "    return df\n",
        "\n",
        "\n",
        "### FOR MAKING A NEW DATA AFTER DROPPING SOME OF THE COLUMNS ###\n",
        "\n",
        "  def newData(self):\n",
        "    df=self.dataManipulation()\n",
        "    df_new=df\n",
        "    df_new.drop(columns=['part_cycle_time','time_in_minute','time_in_second','cumsum_in_minute','part_ID','program_number'],inplace=True)\n",
        "    df_new.drop(columns=['part_cycle_time_with_date','part_count','running_tool_number','tool_changed','material'],inplace=True)\n",
        "    return df_new\n",
        "\n",
        "  # def normalizedData(self):\n",
        "  #   df=self.newData()\n",
        "  #   d_var=df.drop(columns=['cumsum_in_hour','quality'])\n",
        "  #   normalized_df=((d_var-d_var.min())/(d_var.max()-d_var.min()))\n",
        "  #   return normalized_df\n",
        "\n",
        "### TO NORMALIZE THE DATA USING MINMAX NORMALIZER FROM SKLEARN LIBRARY ###\n",
        "## USE AS DEFAULT\n",
        "\n",
        "  def normalizedData1(self):\n",
        "    df=self.newData()\n",
        "    #df.drop(columns=['cutting_speed'],inplace=True)\n",
        "    minmax = MinMaxScaler()\n",
        "    for x in df.columns:\n",
        "      if (x!='cumsum_in_hour' and x!='quality'):\n",
        "        df[x] = minmax.fit_transform(np.array(df[x]).reshape(-1,1))\n",
        "    #df.style.background_gradient(cmap='Blues')\n",
        "    #df.set_index('cumsum_in_hour',inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "### TO NORMALIZE THE DATA USING ROBUST SCALER FROM SKLEARN LIBRARY ###\n",
        "### USE WHEN DATA HAS OUTLIERS\n",
        "\n",
        "  def normalizedData2(self):\n",
        "    df=self.newData()\n",
        "    #df.drop(columns=['cutting_speed'],inplace=True)\n",
        "    robustscaler=RobustScaler()\n",
        "    for x in df.columns:\n",
        "      if (x!='cumsum_in_hour' and x!='quality'):\n",
        "        df[x] = robustscaler.fit_transform(np.array(df[x]).reshape(-1,1))\n",
        "    #df.style.background_gradient(cmap='Blues')\n",
        "    #df.set_index('cumsum_in_hour',inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## cutting speed, depth of cut, diameter, and tolerance were removed as features\n",
        "## as they had outliers greater than 45%\n",
        "  def normalizedData(self):\n",
        "    df=self.normalizedData2()\n",
        "    df.set_index('cumsum_in_hour',inplace=True)\n",
        "    df.drop(columns=['cutting_speed','depth_of_cut','diameter','tolerance','flow_rate','noise','spindle_speed'],inplace=True)\n",
        "    return df\n",
        "\n",
        "  def dataDivPart(self):\n",
        "    df=self.normalizedData()\n",
        "    #df.set_index('cumsum_in_hour',inplace=True)\n",
        "    np.random.seed(1)\n",
        "    totalItem=1000\n",
        "    randomNum1=list(np.random.randint(60,1800,totalItem))\n",
        "    #randomList=list(sum(randomNum1[0:x+1] for x in range (len(randomNum1))))\n",
        "    randomList=[sum(randomNum1[0:x+1]) for x in range(0,len(randomNum1))]\n",
        "    df_div=[]\n",
        "    index=0\n",
        "    for i,val in enumerate(randomList):\n",
        "      df_div.append(df.iloc[index:val,:])\n",
        "      index=val \n",
        "    return df_div\n",
        "\n",
        "\n",
        "\n",
        "### this had the part_cycle_time column extra as compared to the previous method\n",
        "  def dataDivPartNew(self):\n",
        "    df=self.normalizedData()\n",
        "    df1=self.loadData()\n",
        "    df.insert(0,'part_cycle_time',df1['part_cycle_time'])\n",
        "    np.random.seed(1)\n",
        "    totalItem=1000\n",
        "    randomNum1=list(np.random.randint(60,1800,totalItem))\n",
        "    #randomList=list(sum(randomNum1[0:x+1] for x in range (len(randomNum1))))\n",
        "    randomList=[sum(randomNum1[0:x+1]) for x in range(0,len(randomNum1))]\n",
        "    df_div_new=[]\n",
        "    index=0\n",
        "    for i,val in enumerate(randomList):\n",
        "      df_div_new.append(df.iloc[index:val,:])\n",
        "      index=val \n",
        "    return df_div_new\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "    \n",
        "\n",
        "\n",
        "### TO GET SOME INFORMATION ABOUT THE VARIOUS COLUMNS IN THE DATA ###\n",
        "\n",
        "  def dataExploration(self):\n",
        "    ## To get the information about the various columns in the dataframe\n",
        "    df=self.dataDivPartNew()[0]\n",
        "    # print(df.info())\n",
        "    # print(\"\\n\")\n",
        "    # print(df.describe())\n",
        "    # print(\"\\n\")\n",
        "    print(df.columns)\n",
        "    # print(\"\\n\")\n",
        "    # print(df.shape)\n",
        "    # print(\"\\n\")\n",
        "    # print(df.describe().shape)\n",
        "    # print(\"\\n\")\n",
        "    # #df['part_ID'].value_counts()\n",
        "    # print(df.isnull().sum())\n",
        "    # print(\"\\n\")\n",
        "    return df\n",
        "\n",
        "\n",
        "### TO MAKE THE LINEPLOT ###\n",
        "\n",
        "  def lineplot(self,z):\n",
        "    df=self.normalizedData()\n",
        "    #df.set_index('cumsum_in_hour',inplace=True) \n",
        "    df.plot(subplots=True,linewidth=2.0,figsize=(30,30))\n",
        "    plt.axis([0,z,0,1])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.figure()\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "  # def lineplot(self,a1,b1,c1,d1):\n",
        "  #   import matplotlib.pyplot as plt\n",
        "  #   import seaborn as sns\n",
        "  #   df=self.dataManipulation()\n",
        "  #   fig, axes = plt.subplots(3,1, figsize=(10,10),sharex=True)\n",
        "  #   # plt.xlim(1,50000)\n",
        "  #   # plt.ylim(1,100)\n",
        "  #   for name, ax in zip([a1,b1,c1], axes):\n",
        "  #     plt.plot(data=df, x=d1, y=name, ax=ax, linewidth=2.0)\n",
        "  #     ax.set_title(name)\n",
        "  #     plt.show()\n",
        "  #   # Remove the automatic x-axis label from all but the bottom subplot\n",
        "  #   if ax != axes[-1]:\n",
        "  #       ax.set_xlabel('')\n",
        "  #   print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # def boxplot(self,a1,b1,c1,d1):\n",
        "  #   import matplotlib.pyplot as plt\n",
        "  #   import seaborn as sns\n",
        "  #   df=self.dataManipulation()\n",
        "  #   fig, axes = plt.subplots(3, 1, figsize=(100, 100), sharex=True)\n",
        "  #   plt.xlim(1,50000)\n",
        "  #   plt.ylim(1,100)\n",
        "  #   for name, ax in zip([a1, b1, c1], axes):\n",
        "  #     sns.boxplot(data=df, x=d1, y=name, ax=ax)\n",
        "  #     ax.set_title(name)\n",
        "  #   # Remove the automatic x-axis label from all but the bottom subplot\n",
        "  #   if ax != axes[-1]:\n",
        "  #       ax.set_xlabel('')\n",
        "  #   print(\"\\n\")\n",
        "\n",
        "\n",
        "### TO MAKE THE BOXPLOT ###\n",
        "\n",
        "  def boxgraph(self):\n",
        "    df=self.normalizedData()\n",
        "    #df.boxplot(figsize=(15,15),grid=True,by='quality')\n",
        "    df.boxplot(figsize=(15,15),grid=True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#### Observations from a box plot \n",
        "### Outliers are data points observed outside the whisker at a certain range   [Q1-1.5*IQR and Q3+1.5*IQR] IQR-> Inter-Quartile Range    (observed in spindle speed, noise, vibration_z and cutting speed)\n",
        "### If the 50 percentile line is not in the middle then the data does not come from a symmetric distributrion\n",
        "### 50 percentile line is closer to the Q1 line it is right-skewed as is oberved in many cases\n",
        "### more the length more the inter-quartile range more is the spread of the distribution\n",
        "\n",
        "\n",
        "  def countOutlier(self):\n",
        "    df=self.normalizedData2()\n",
        "    df.drop(columns=['quality','cumsum_in_hour'],inplace=True)\n",
        "    pos=[]\n",
        "    count=0\n",
        "    for i in df.columns:\n",
        "      x=df.loc[:,i]\n",
        "      iqr=np.subtract(*np.percentile(x, [75, 25]))\n",
        "      q1=np.percentile(x,25)\n",
        "      q3=np.percentile(x,75)\n",
        "      o1=q1-(1.5*iqr)\n",
        "      o2=q3+(1.5*iqr)\n",
        "      for j in range(939284):\n",
        "        if ((x[j]<o1) | (x[j]>o2)):\n",
        "          pos.append(True)   ## outliers would be assigned a value true\n",
        "          count=count+1\n",
        "        else:\n",
        "          pos.append(False)\n",
        "      print(f\"The column {i} has {(count/939284)*100}% outlier\")\n",
        "    \n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### TO MAKE THE HISTOGRAM ###\n",
        "  def histogram(self):\n",
        "    df=self.normalizedData()\n",
        "    plt.figure(figsize=(20,20))\n",
        "    #df.set_index('cumsum_in_hour',inplace=True)\n",
        "    df.hist(grid=True,bins=15,color='steelblue', edgecolor='black', linewidth=2.0,\n",
        "           xlabelsize=10, ylabelsize=10)\n",
        "    plt.tight_layout(rect=(0, 0, 1.2, 1.2)) \n",
        "    # plt.tick_params(axis='x',labelsize=10)\n",
        "    # plt.tick_params(axis='y',labelsize=10)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "### TO GET THE PEARSON AND SPEARMAN RANK CORRELATION COEFFICIENT ###\n",
        "  def correlationCoefficient(self):\n",
        "    df=self.normalizedData()\n",
        "    #df.set_index(\"cumsum_in_hour\",inplace=True)\n",
        "    corr=df.corr(method='pearson',min_periods=1)\n",
        "    corr1=df.corr(method='spearman',min_periods=1)\n",
        "    return corr,corr1\n",
        "\n",
        "\n",
        "# Correlation Matrix Heatmap\n",
        "  def correlationHeatMap(self):\n",
        "    df=self.normalizedData()\n",
        "    #df.set_index(\"cumsum_in_hour\",inplace=True)\n",
        "    f, ax = plt.subplots(figsize=(10, 6))\n",
        "    corr = df.corr(method=\"pearson\")\n",
        "    sns.heatmap(round(corr,2), annot=True, ax=ax, cmap=\"coolwarm\",fmt='.2f',\n",
        "                    linewidths=.05)\n",
        "    f.subplots_adjust(top=0.93)\n",
        "    f.suptitle('Pearson Correlation Heatmap', fontsize=14)\n",
        "\n",
        "\n",
        "### TO GET THE HEAT MAP BUT COULD NOT USE IT AS IT COULD A LOT OF TIME ###\n",
        "\n",
        "  def heatmap(self):\n",
        "    df=self.normalizedData()\n",
        "    #df.set_index(\"cumsum_in_hour\",inplace=True)\n",
        "    df.drop(columns=['quality'],inplace=True)\n",
        "    sns.heatmap(df,annot=True,cmap='RdYlGn', linewidths=0.5)\n",
        "\n",
        "### TO GET THE DISTRIBUTION PLOT BUT COULD NOT USE IT AS IT COULD A LOT OF TIME ###\n",
        "  def distplot(self,x):\n",
        "    df=self.newData()\n",
        "    sns.distplot(df[x])\n",
        "    #sns.kdeplot(df[x], shade=True, color='steelblue')\n",
        "    \n",
        "\n",
        "# Pair-wise Scatter Plots\n",
        "  def pairwiseScatter(self):\n",
        "    df=self.normalizedData()\n",
        "    \n",
        "    cols = ['spindle_load_%', 'current',\n",
        "        'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "        'vibration_spindle']\n",
        "    #f, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.pairplot(df[cols], height=1.8, aspect=1.8,\n",
        "                      plot_kws=dict(edgecolor=\"k\", linewidth=0.5),\n",
        "                      diag_kind=\"kde\", diag_kws=dict(shade=True))\n",
        "\n",
        "    \n",
        "    # f.subplots_adjust(top=0.93, wspace=0.3)\n",
        "    # f.suptitle('Pairwise Plots', fontsize=14)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# https://adtk.readthedocs.io/en/stable/quickstart.html\n",
        "  def adtk(self,i):\n",
        "    df=self.dataDivPartNew()\n",
        "    df[i].drop(columns=['cumsum_in_hour','quality'],inplace=True)\n",
        "    df[i].set_index(\"part_cycle_time\",inplace=True)\n",
        "    # series=validate_series(df[i])\n",
        "    # plot(series)\n",
        "\n",
        "    seasonal_ad = SeasonalAD()\n",
        "    anomalies = seasonal_ad.fit_detect(df[i])\n",
        "    plot(s_train, anomaly=anomalies, anomaly_color=\"red\", anomaly_tag=\"marker\")\n",
        "\n",
        "\n",
        "############## k-means algorithm ##################################\n",
        "###################################################################\n",
        "\n",
        "# k-means is a widely used clustering algorithm. \n",
        "# It creates ‘k’ similar clusters of data points. \n",
        "# Data instances that fall outside of these groups \n",
        "# could potentially be marked as anomalies. \n",
        "# Before we start k-means clustering, \n",
        "# we use elbow method to determine the optimal number of clusters.\n",
        "# Reference: https://towardsdatascience.com/time-series-of-price-anomaly-detection-13586cd5ff46\n",
        "\n",
        "\n",
        "\n",
        "# Now to find out the number of components (features) to keep\n",
        "## is done using PCA\n",
        "## Standard Scaler not used because the sns.distplot or the distribution plot did not show a normal distribution\n",
        "  def pca(self):\n",
        "    df=self.normalizedData()\n",
        "    data = df[['spindle_load_%', 'current',\n",
        "        'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "        'vibration_spindle']]\n",
        "    X_std = data.values\n",
        "    mean_vec = np.mean(X_std, axis=0)\n",
        "    cov_mat = np.cov(X_std.T)\n",
        "    eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
        "    eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
        "    eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
        "    tot = sum(eig_vals)\n",
        "    var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
        "    cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(range(len(var_exp)), var_exp, alpha=0.3, align='center', label='individual explained variance', color = 'g')\n",
        "    plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')\n",
        "    plt.ylabel('Explained variance ratio')\n",
        "    plt.xlabel('Principal components')\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  def pca1(self):\n",
        "    df=self.normalizedData1()\n",
        "    # Take useful feature and standardize them\n",
        "    data = df[['spindle_load_%', 'current',\n",
        "        'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "        'vibration_spindle']]\n",
        "    X = data.values\n",
        "    #X_std = StandardScaler().fit_transform(X)\n",
        "    # reduce to 2 important features\n",
        "    pca = PCA(n_components=2)\n",
        "    data_new = pca.fit_transform(X)\n",
        "    # standardize these 2 new features\n",
        "    # scaler = StandardScaler()\n",
        "    # np_scaled = scaler.fit_transform(data)\n",
        "    # df['quality'].fillna(1,inplace=True)\n",
        "    # df['quality'].replace({'ACCEPTED':1,'REJECTED':0},inplace=True)\n",
        "    l=[np.repeat(1,(939284-3700))]\n",
        "    l.append(np.repeat(0,3700))\n",
        "    l1=list(itertools.chain.from_iterable(l))\n",
        "    np.random.seed(1)\n",
        "    q=random.sample(l1,len(l1))\n",
        "    pcaDataframe = pd.DataFrame(data_new,columns = ['principal_component_1', 'principal_component_2'])\n",
        "    pcaData=pcaDataframe\n",
        "    pcaDataframe['quality']=q\n",
        "    #print(pcaDataframe)\n",
        "##########################################################\n",
        "    # fig = plt.figure(figsize = (15,15))\n",
        "    # plt.scatter(data_new[:, 0], data_new[:, 1],\n",
        "    #         c=pcaDataframe.quality, edgecolor='none', alpha=0.5,\n",
        "    #         cmap=plt.cm.get_cmap('Blues', 10))\n",
        "    # plt.axis([0,0.2,0,0.2])\n",
        "    # plt.xlabel('component 1')\n",
        "    # plt.ylabel('component 2')\n",
        "    # plt.colorbar()\n",
        "    # plt.show()\n",
        "#############################################\n",
        "    # fig = plt.figure(figsize = (15,15))\n",
        "    # ax = fig.add_subplot(1,1,1) \n",
        "    # # Setting X-axis and Y-axis limits\n",
        "    # ax.set_xlim([0.0, 0.05])\n",
        "    # ax.set_ylim([0, 0.05])\n",
        "    # ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "    # ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "    # ax.set_title('2 component PCA', fontsize = 20)\n",
        "    # targets = [1, 0]\n",
        "    # colors = ['r', 'b']\n",
        "    # for target, color in zip(targets,colors):\n",
        "    #   indicesToKeep = pcaDataframe['quality'] == target\n",
        "    #   ax.scatter(pcaDataframe.loc[indicesToKeep, 'principal_component_1']\n",
        "    #               , pcaDataframe.loc[indicesToKeep, 'principal_component_2']\n",
        "    #               , c = color\n",
        "    #               , s = 50)\n",
        "    # ax.legend(targets)\n",
        "    # ax.grid()\n",
        "    # plt.show()\n",
        "    return pcaData\n",
        "\n",
        "\n",
        "### took a long time to run \n",
        "\n",
        "  def tsne(self):\n",
        "    df=self.normalizedData()\n",
        "    data = df[['spindle_load_%', 'current',\n",
        "        'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "        'vibration_spindle']]\n",
        "    X = data.values\n",
        "    t_sne=TSNE(n_components=2)\n",
        "    X_embedded = t_sne.fit_transform(X)\n",
        "    X_embedded.shape\n",
        "    tsneDataframe=pd.DataFrame(X_embedded,columns=['component_1','component_2'])\n",
        "    print(tsneDataframe)\n",
        "    plt.scatter(X_embedded[:0],X_embedded[:1])\n",
        "    plt.show()\n",
        "    return tsneDataframe\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # def elbowplot(self):\n",
        "  #   df=self.normalizedData()\n",
        "  #   data = df[['spindle_load_%', 'current', \n",
        "  #      'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "  #      'vibration_spindle', 'flow_rate', \n",
        "  #      'depth_of_cut', 'diameter', 'tolerance']].values\n",
        "  #   n_cluster = range(1, 15)\n",
        "  #   kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\n",
        "  #   scores = [kmeans[i].score(data) for i in range(len(kmeans))]\n",
        "\n",
        "  #   fig, ax = plt.subplots(figsize=(10,6))\n",
        "  #   ax.plot(n_cluster, scores)\n",
        "  #   plt.xlabel('Number of Clusters')\n",
        "  #   plt.ylabel('Score')\n",
        "  #   plt.title('Elbow Curve')\n",
        "  #   plt.show()\n",
        "\n",
        "\n",
        "### this is the elbow plot to determine the optimal number of clusters \n",
        "### to be considered \n",
        "### Here the sum of squared distances (inertia) is plotted against\n",
        "### the number of clusters \n",
        "### the elbow point determines the optimal number of clusters\n",
        "  def elbowplot1(self):\n",
        "    df=self.normalizedData()\n",
        "    data = df[['spindle_load_%', 'current',\n",
        "        'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "        'vibration_spindle']].values\n",
        "    Sum_of_squared_distances = []\n",
        "    K = range(1,15)\n",
        "    for k in K:\n",
        "      km = KMeans(n_clusters=k)\n",
        "      km = km.fit(data)\n",
        "      Sum_of_squared_distances.append(km.inertia_)\n",
        "    plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
        "    plt.xlabel('Number of clusters (k)')\n",
        "    plt.ylabel('Sum_of_squared_distances')\n",
        "    plt.title('Elbow Method For Optimal k')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "## silhouette score vary between -1 to +1 so closer it is to 1 more better it is\n",
        "## the peak value which is closer to one is the ideal cluster value\n",
        "###### took a long time to run ##########################\n",
        "\n",
        "  def silhouetteScore(self):\n",
        "    df=self.normalizedData()\n",
        "    data = df[['spindle_load_%', 'current',\n",
        "        'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "        'vibration_spindle']].values\n",
        "    sil = []\n",
        "    kmax = 6\n",
        "    K=range(2,kmax+1)\n",
        "    # dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2\n",
        "    for k in K:\n",
        "      kmeans = KMeans(n_clusters = k).fit(data)\n",
        "      labels = kmeans.labels_\n",
        "      sil.append(silhouette_score(data, labels, metric = 'euclidean'))\n",
        "      print(sil)\n",
        "      print(\"\\n\")\n",
        "    plt.plot(K, sil, 'bx-')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('Silhouette score')\n",
        "    plt.title('Silhouette score method For Optimal k')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "## from the elbow plot 1 it can be seen that the plot levels off after 3 that is the elbow point is at 3 so here n_cluster=3\n",
        "## n_cluster represent the number of clusters as well as the number of centroids generated\n",
        "## here y_means will show which training example belong to which cluster\n",
        "\n",
        "### As the ground truth or the actual labels were not available so the metrics used were those which did \n",
        "### not require ground truth labels\n",
        "\n",
        "  def kmeans(self):\n",
        "    # df=self.normalizedData()\n",
        "    # X = df[['spindle_load_%', 'current',\n",
        "    #     'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "    #     'vibration_spindle']].values\n",
        "    pca=self.pca1()\n",
        "    X=pca.values\n",
        "    km = KMeans(n_clusters=3,random_state=0)\n",
        "    km.fit(X)\n",
        "    y_means=km.predict(X)\n",
        "    labels = km.labels_\n",
        "    print(\"The labels of each point\")\n",
        "    #print(labels)\n",
        "    print(\"The value of y_means is \\n\")\n",
        "    print(y_means)\n",
        "    print(\"Sum of squared distances of samples to their closest cluster center is\\n\")\n",
        "    print(km.inertia_)\n",
        "    print(\"The value of the coordinates of the cluster center\\n\")\n",
        "    print(km.cluster_centers_)\n",
        "    print(\"The number of iterations run is\\n\")\n",
        "    print(km.n_iter_)\n",
        "    print(km.get_params)\n",
        "    print(f\"The shape of X is {X.shape}\")\n",
        "    # score_silhouette=metrics.silhouette_score(X, labels, metric='euclidean')\n",
        "    # print(f\"The silhouette score is {score_silhouette}\")\n",
        "    score_calinski=metrics.calinski_harabasz_score(X, labels)\n",
        "    print(f\"The Calinski-Harabasz Index is {score_calinski}\")\n",
        "    score_davies=metrics.davies_bouldin_score(X, labels)\n",
        "    print(f\"The Davies_Bouldin Index is {score_davies}\")\n",
        "\n",
        "    #Plotting\n",
        "    # plt.scatter(X[y_means==0, 0], X[y_means==0, 1], s=100,  c='red', label ='Cluster 1')\n",
        "    # plt.scatter(X[y_means==1, 0], X[y_means==1, 1], s=100, c='blue', label ='Cluster 2')\n",
        "    # plt.scatter(X[y_means==2, 0], X[y_means==2, 1], s=100,  c='green', label ='Cluster 3')\n",
        "    # plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=300, c='yellow', label = 'Centroids')\n",
        "    # #plt.axis([0.125,0.175,0.25,0.75])\n",
        "    # # #Axes3D.scatter(, ys, zs=0, zdir='z', s=20, c=None, depthshade=True, *args, **kwargs)\n",
        "    # # Axes3D.scatter(xs=X[y_means==0, 0], ys=X[y_means==0, 1], zs=0, zdir='z',s=100,  c='red', label ='Cluster 1')\n",
        "    # # Axes3D.scatter(xs=X[y_means==1, 0], ys=X[y_means==1, 1], zs=0, zdir='z', s=100, c='blue', label ='Cluster 2')\n",
        "    # # Axes3D.scatter(xs=X[y_means==2, 0], ys=X[y_means==2, 1], zs=0, zdir='z', s=100,  c='green', label ='Cluster 3')\n",
        "    # # Axes3D.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], zs=0, zdir='z', s=300, c='yellow', label = 'Centroids')\n",
        "    # plt.title('KMeans')\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Mini-Batch K-Means clustering\n",
        "########### took a long time to calculate the parameters ##################\n",
        "\n",
        "  def miniBatchKmeans(self):\n",
        "    # df=self.normalizedData()\n",
        "    # X = df[['spindle_load_%', 'current',\n",
        "    #     'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "    #     'vibration_spindle']].values\n",
        "    pca=self.pca1()\n",
        "    X=pca.values\n",
        "    km = MiniBatchKMeans(n_clusters=3,random_state=0)\n",
        "    km.fit(X)\n",
        "    y_means=km.predict(X)\n",
        "    labels = km.labels_\n",
        "    #print(\"The labels of each point\")\n",
        "    #print(labels)\n",
        "    print(\"The value of y_means is \\n\")\n",
        "    print(y_means)\n",
        "    print(\"Sum of squared distances of samples to their closest cluster center is\\n\")\n",
        "    print(km.inertia_)\n",
        "    print(\"The value of the coordinates of the cluster center\\n\")\n",
        "    print(km.cluster_centers_)\n",
        "    print(\"The number of iterations run is\\n\")\n",
        "    print(km.n_iter_)\n",
        "    print(km.get_params)\n",
        "    score_silhouette=metrics.silhouette_score(X, labels, metric='euclidean')\n",
        "    print(f\"The silhouette score is {score_silhouette}\")\n",
        "    score_calinski=metrics.calinski_harabasz_score(X, labels)\n",
        "    print(f\"The Calinski-Harabasz Index is {score_calinski}\")\n",
        "    score_davies=metrics.davies_bouldin_score(X, labels)\n",
        "    print(f\"The Davies_Bouldin Index is {score_davies}\")\n",
        "    #Plotting\n",
        "    # fig = plt.figure(1, figsize=(30,30))\n",
        "    # ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)\n",
        "    # ax.scatter(X.iloc[:,0], X.iloc[:,4], X.iloc[:,3],\n",
        "    #           c=labels.astype(np.float), edgecolor=\"k\")\n",
        "    # plt.title(\"K Means\", fontsize=14)\n",
        "    #plt.scatter(X.iloc[:, 0], X.iloc[:, 3])\n",
        "    #Plotting\n",
        "    plt.scatter(X[y_means==0, 0], X[y_means==0, 1], s=100,  c='red', label ='Cluster 1')\n",
        "    plt.scatter(X[y_means==1, 0], X[y_means==1, 1], s=100, c='blue', label ='Cluster 2')\n",
        "    plt.scatter(X[y_means==2, 0], X[y_means==2, 1], s=100,  c='green', label ='Cluster 3')\n",
        "    plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=300, c='yellow', label = 'Centroids')\n",
        "    #plt.axis([0.125,0.175,0.25,0.75])\n",
        "    plt.title('KMeans')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  def dbscan(self):\n",
        "    pca=self.pca1()\n",
        "    X=pca.values\n",
        "    db_scan=DBSCAN(eps=3, min_samples=100000)\n",
        "    labels=db_scan.fit_predict(X)\n",
        "    print(labels)\n",
        "    print(db_scan.get_params)\n",
        "    score_silhouette=metrics.silhouette_score(X, labels, metric='euclidean')\n",
        "    print(f\"The silhouette score is {score_silhouette}\")\n",
        "    score_calinski=metrics.calinski_harabasz_score(X, labels)\n",
        "    print(f\"The Calinski-Harabasz Index is {score_calinski}\")\n",
        "    score_davies=metrics.davies_bouldin_score(X, labels)\n",
        "    print(f\"The Davies_Bouldin Index is {score_davies}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# To get the count of the Accepted and Rejected parts\n",
        "  def count(self):\n",
        "    df=self.loadData()\n",
        "    df['quality'].replace({\"ACCEPTED\":1,\"REJECTED\":0},inplace=True)\n",
        "    df['quality'].dropna(inplace=True)\n",
        "    #print(df['quality'].isnull().sum())\n",
        "    neg, pos = np.bincount(df['quality'])\n",
        "    total = neg + pos\n",
        "    print('Examples:\\n    Total: {}\\n    Negative: {} ({:.2f}% of total)\\n'.format(\n",
        "        total, neg, 100 * neg / total))\n",
        "\n",
        "\n",
        "# The whole dataframe was divided into train, test and validation set\n",
        "# The validataion set is used during the model fitting to evaluate the loss and any metrics. \n",
        "# The test set is completely unused during the training phase and is only used at the end to evaluate \n",
        "# how well the model is performing on unseen data.\n",
        "# This is especially important in case of imbalanced data where overfitting is a significant concern. \n",
        "\n",
        "\n",
        "  def trainTestSplit(self):\n",
        "    df=self.dataDivPart()\n",
        "    for i in range(len(df)):\n",
        "      df[i]['quality'].replace({\"ACCEPTED\":1,\"REJECTED\":0},inplace=True)\n",
        "      df[i]['quality'].dropna(inplace=True)\n",
        "    \n",
        "    train, test = train_test_split(df, test_size=0.2,random_state=0)\n",
        "    train, val = train_test_split(train, test_size=0.2,random_state=0)\n",
        "    print(type(train))\n",
        "\n",
        "    # print(len(train), 'train examples')\n",
        "    # print(len(val), 'validation examples')\n",
        "    # print(len(test), 'test examples')\n",
        "    return train,val,test\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# Form np arrays of labels and features.\n",
        "  def train_labels_features(self):\n",
        "    train,val,test=self.trainTestSplit()\n",
        "    train_labels_list=[]\n",
        "    train_features_list=[]\n",
        "    for i in range(len(train)):\n",
        "      train_labels_list.append(train[i].loc[:,'quality'].values.tolist())\n",
        "      train[i].drop(columns=['quality'],inplace=True)\n",
        "      train_features_list.append(train[i].values)\n",
        "    train_labels=[item for elem in train_labels_list for item in elem]\n",
        "    train_labels=np.array(train_labels)\n",
        "    train_f1=train_features_list[0:80]\n",
        "    train_f2=train_features_list[80:160]\n",
        "    train_f3=train_features_list[160:240]\n",
        "    train_f4=train_features_list[240:320]\n",
        "    train_f5=train_features_list[320:400]\n",
        "    train_f6=train_features_list[400:480]\n",
        "    train_f7=train_features_list[480:560]\n",
        "    train_f8=train_features_list[560:]\n",
        "    \n",
        "    def func(train_features):\n",
        "      import functools\n",
        "      import operator\n",
        "      output=[]\n",
        "      for i in train_features:\n",
        "        List_flat = functools.reduce(operator.iconcat, i,[])\n",
        "        output.append(List_flat)\n",
        "      return output\n",
        "    \n",
        "    output1=func(train_f1)\n",
        "    output2=func(train_f2)\n",
        "    output3=func(train_f3)\n",
        "    output4=func(train_f4)\n",
        "    output5=func(train_f5)\n",
        "    output6=func(train_f6)\n",
        "    output7=func(train_f7)\n",
        "    output8=func(train_f8)   \n",
        "    output=output1+output2+output3+output4+output5+output6+output7+output8\n",
        "    train_features=np.array(output)\n",
        "    print(train_features[0])\n",
        "    return train_labels,train_features\n",
        "\n",
        "     \n",
        "  \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "  def val_labels_features(self):\n",
        "    train,val,test=self.trainTestSplit()\n",
        "    val_labels_list=[]\n",
        "    val_features_list=[]\n",
        "    val_list=[]\n",
        "    len_val=[]\n",
        "    for i in range(len(val)):\n",
        "      val_labels_list.append(val[i].loc[:,'quality'].values.tolist())\n",
        "      val[i].drop(columns=['quality'],inplace=True)\n",
        "      for elem in val[i].values:\n",
        "        for item in elem:\n",
        "          val_list.append(item)\n",
        "\n",
        "      val_features_list.append(val_list)\n",
        "      len_val.append(len(val_features_list[i]))\n",
        "    val_features=np.array(val_features_list)\n",
        "    val_labels=[item for elem in val_labels_list for item in elem ]\n",
        "    val_labels=np.array(val_labels)\n",
        "    #print(\"\\n The val labels are\\n\")\n",
        "    #print(len(val_labels))\n",
        "    # print(\"\\n The val features are\\n\")\n",
        "    # print(val_features)\n",
        "    print(len(len_val))\n",
        "    print(\"\\n\")\n",
        "    print(len_val)\n",
        "    \n",
        "    return val_labels,val_features\n",
        "\n",
        "\n",
        "\n",
        "  def test_labels_features(self):\n",
        "    train,val,test=self.trainTestSplit()\n",
        "    test_labels_list=[]\n",
        "    test_features_list=[]\n",
        "    test_list=[]\n",
        "    len_test=[]\n",
        "    for i in range(len(test)):\n",
        "      test_labels_list.append(test[i].loc[:,'quality'].values.tolist())\n",
        "      test[i].drop(columns=['quality'],inplace=True)\n",
        "      for elem in test[i].values:\n",
        "        for item in elem:\n",
        "          test_list.append(item)\n",
        "\n",
        "      test_features_list.append(test_list)\n",
        "      len_test.append(len(test_features_list[i]))\n",
        "    test_features=np.array(test_features_list)\n",
        "    test_labels=[item for elem in test_labels_list for item in elem ]\n",
        "    test_labels=np.array(test_labels)\n",
        "    # print(\"\\n The test labels are\\n\")\n",
        "    # print(len(test_labels))\n",
        "    # print(\"\\n The test features are\\n\")\n",
        "    # print(test_features)\n",
        "    print(len(len_test))\n",
        "    print(\"\\n\")\n",
        "    print(len_test)\n",
        "    \n",
        "    return test_labels,test_features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### MAIN FUNCTION ######\n",
        "\n",
        "def main():\n",
        "  a=AnomalyDetection()\n",
        "  #print(a.loadData())\n",
        "  # print(\"\\n\")\n",
        "  #print(a.dataManipulation())\n",
        "  # print(\"\\n\")\n",
        "  #print(a.newData().shape)\n",
        "  # print(\"The normalized data with MinMaxScaler\\n\")\n",
        "  # print(a.normalizedData1())\n",
        "  # print(\"The normalized data with RobustScaler\\n\")\n",
        "  # print(a.normalizedData2())\n",
        "  # print(a.dataDivPart())\n",
        "  # print(a.dataDivPartNew())\n",
        "  #a.dataExploration()\n",
        "  # print(\"\\n\")\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  #a.lineplot(4000)\n",
        "  #print(\"\\n\")\n",
        "  #a.lineplot('spindle_speed','cumsum_in_hour',1500)\n",
        "  #a.boxgraph()\n",
        "  #a.countOutlier()\n",
        "  #print(\"\\n\")\n",
        "  #a.histogram()\n",
        " \n",
        "  # print(\"The pearson coefficient is:\")\n",
        "  # print(a.correlationCoefficient()[0])\n",
        "  # print(\"\\n\")\n",
        "  # print(\"The spearman coefficient is:\")\n",
        "  # print(a.correlationCoefficient()[1])\n",
        "  # a.correlationHeatMap()\n",
        "  #a.heatmap()\n",
        "  # cols=['spindle_load_%', 'current', 'spindle_speed',\n",
        "  #         'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "  #         'vibration_spindle', 'noise', 'flow_rate', 'cutting_speed',\n",
        "  #         'depth_of_cut', 'diameter', 'tolerance']\n",
        "  # for i in cols:  \n",
        "  #   a.distplot(i)\n",
        "  #   plt.show()\n",
        "  # a.pairwiseScatter()\n",
        "  #a.adtk(700)\n",
        "  #a.pca()\n",
        "  #a.pca1()\n",
        "  #a.tsne()\n",
        "  #a.elbowplot()\n",
        "  a.elbowplot1()\n",
        "  #a.silhouetteScore()\n",
        "  #a.kmeans()\n",
        "  # print(\"Mini Batch KMeans below\\n\")\n",
        "  #a.miniBatchKmeans()\n",
        "  #a.dbscan()\n",
        "  #a.trainTestSplit()\n",
        "  # print(\"\\n\")\n",
        "  #a.count()\n",
        "  #a.train_labels_features()\n",
        "  #a.val_labels_features()\n",
        "  #a.test_labels_features()\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  main()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJgzjU6d3xn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class MongoDB(object):\n",
        "#   def __init__(self, dBName=None, collectionName=None):\n",
        "#     self.dBName=dBName\n",
        "#     self.collectionName= collectionName\n",
        "#     self.client=MongoClient(\"localhost\", 27017)\n",
        "#     self.DB=self.client[self.dBName]\n",
        "#     self.collection=self.DB[self.collectionName]\n",
        "\n",
        "#   def InsertData(self,path=None):\n",
        "\n",
        "#     copied_path=(\"Data_final_version.csv\")\n",
        "#     df=pd.read_csv(copied_path)\n",
        "#     data=df.to_dict('records')\n",
        "\n",
        "#     self.collection.insert_many(data,ordered=False)\n",
        "#     print(\"All the data has been exported to mongoDB server\")\n",
        "\n",
        "# if __name__==\"__main__\":\n",
        "#   mongodb=MongoDB(dBName='DataIntern',collectionName='AnomalyDetect')\n",
        "#   mongodb.InsertData(path=\"Data_final_version.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBh5Mk0E5LQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7360ea5a-4167-4330-e7eb-da1f38166ec4"
      },
      "source": [
        "# import functools\n",
        "# import operator\n",
        "# l=[[[1,2,3],[4,5,6],[7,8,9]],[[10,11,12],[13,14,15],[16,17,18]],[[19,20,21],[22,23,24],[25,26,27]]]   # list to be flattened\n",
        "\n",
        "# output=[]\n",
        "# for i in range(len(l)):\n",
        "#   List_flat = functools.reduce(operator.iconcat, l[i],[])\n",
        "#   output.append(List_flat)\n",
        "\n",
        "# print(\"Original List:\",l)\n",
        "# print(\"Flattened List:\",output)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original List: [[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24], [25, 26, 27]]]\n",
            "Flattened List: [[1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18], [19, 20, 21, 22, 23, 24, 25, 26, 27]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwzfue6GRZ_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2a7c4e85-ac99-43d6-babd-7f280864db10"
      },
      "source": [
        "# import itertools\n",
        "# l=[[[1,2,3],[4,5,6],[7,8,9]],[[10,11,12],[13,14,15],[16,17,18]],[[19,20,21],[22,23,24],[25,26,27]]]\n",
        "\n",
        "# output=[]\n",
        "# for i in range(len(l)):\n",
        "#   List_flat = list(itertools.chain(*l[i]))\n",
        "#   output.append(List_flat)\n",
        "\n",
        "\n",
        "# print(\"Original List:\",l)\n",
        "# print(\"Flattened List:\",output)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original List: [[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24], [25, 26, 27]]]\n",
            "Flattened List: [[1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18], [19, 20, 21, 22, 23, 24, 25, 26, 27]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t83bHywsS8LL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}