{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_final_29_07_20.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1S_XkkvqF1_Si4h3yGLHgmpIhOMXFh5A7",
      "authorship_tag": "ABX9TyMTwnyqACehVfNon2JOuo2i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayantikag98/Internship/blob/master/ML_final_29_07_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsgKD_kEgUqT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "97784775-9705-4064-9f9d-6d6a81aebd0c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbR3Yrm6gmKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the different libraries\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#! pip install adtk\n",
        "from adtk.data import validate_series\n",
        "from adtk.visualization import plot\n",
        "from adtk.detector import SeasonalAD\n",
        "from adtk.data import to_events\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.mplot3d import Axes3D  \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os \n",
        "import tempfile\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "import random\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import Birch\n",
        "import functools\n",
        "import operator\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Conv2D, Conv1D, Flatten\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.utils import resample\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.under_sampling import ClusterCentroids\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTETomek\n",
        "from sklearn.utils import resample\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import RepeatedKFold"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6swwaDFVyhmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "f9bc82cc-70aa-42fc-b392-c1680077b95d"
      },
      "source": [
        "### Approx 100 machines and 1 day data\n",
        "\n",
        "    \n",
        "class AnomalyDetection:\n",
        "\n",
        "  ### INITIATOR ###\n",
        "\n",
        "  def __init__(self):\n",
        "    # ! pip install pymongo\n",
        "    # try:\n",
        "      # import pymongo\n",
        "      # from pymongo import MongoClient\n",
        "      # import json\n",
        "    # except Exception as e:\n",
        "    #   print(\"Some modules are missing\")\n",
        "    pass\n",
        "    \n",
        "\n",
        "\n",
        " ### LOAD DATA ###\n",
        "\n",
        "  def loadData (self):\n",
        "    copied_path=\"drive/My Drive/Data_final_version.csv\"\n",
        "    data=pd.read_csv(copied_path,parse_dates=[\"part_cycle_time\"])\n",
        "    #print(data.dtypes)\n",
        "    return data\n",
        "\n",
        "    ### CODE TO IMPORT DATA STORED IN MONGODB\n",
        "\n",
        "  #def importData(self):\n",
        "    ## To import the data from MongoDB server as pandas DataFrame\n",
        "    # client = MongoClient()\n",
        "    # db = client.DataIntern\n",
        "    # collection = db.AnomalyDetect\n",
        "    # data = pd.DataFrame(list(collection.find()))\n",
        "    # return data\n",
        "\n",
        "### TO MAKE SOME CHANGES IN SOME PARTICULAR COLUMNS ###\n",
        "\n",
        "  # def columnManipulation(self,df):\n",
        "  #   ## To correct the values of certain columns\n",
        "  #   import numpy as np\n",
        "  #   import itertools\n",
        "  #   np.random.seed(1)\n",
        "  #   totalItem=1000\n",
        "  #   randomNum1=np.random.randint(60,1800,totalItem)\n",
        "  #   randomList=list(randomNum1)\n",
        "  #   # to generate 1000 evenly spaced numbers between 1 and 1000\n",
        "  #   ranList1=np.linspace(1,totalItem,totalItem,dtype=\"int\")\n",
        "  #   ranList2=np.random.randint(1,200,totalItem)\n",
        "  #   #print(ranList2)\n",
        "  #   # to make the part_count and part_ID column \n",
        "  #   lis=[]\n",
        "  #   lis1=[]\n",
        "  #   for i in range(len(randomList)):\n",
        "  #       lis.append(np.repeat(ranList1[i],randomList[i]))\n",
        "  #       lis1.append(np.repeat(ranList2[i],randomList[i]))\n",
        "  #   df['part_count']=list(itertools.chain.from_iterable(lis))\n",
        "  #   df['part_ID']=list(itertools.chain.from_iterable(lis1))  \n",
        "  #   return df\n",
        "\n",
        "  \n",
        "  ### TO IMPUTE USING KNN IMPUTER BUT DID NOT USE ###\n",
        "\n",
        "  # def knnImpute(self,df,column):\n",
        "  #   import numpy as np\n",
        "  #   from sklearn.impute import KNNImputer\n",
        "  #   x=df[column].values\n",
        "  #   imputer= KNNImputer(n_neighbors=2)\n",
        "  #   print(\"\\n\")\n",
        "  #   print(\"The result after KNN Imputation\")\n",
        "  #   print(imputer.fit_transform(x.reshape(-1,1)))\n",
        "  #   print(\"\\n\")\n",
        "\n",
        "\n",
        "### FOR MAKING SOME CHANGES IN SOME OF THE COLUMNS IN THE DATA ###\n",
        "\n",
        "  def dataManipulation(self):\n",
        "    ## To remove the unneccessary columns and to store a copy of the dataframe\n",
        "    ## To create only time and time_in_second column\n",
        "    #print(data['_id'].nunique())\n",
        "    data=self.loadData()\n",
        "    df1=data\n",
        "    df=data\n",
        "    df['part_cycle_time']=df1['part_cycle_time'].dt.time\n",
        "    data1=self.loadData()\n",
        "    data2=self.loadData()\n",
        "    data3=self.loadData()\n",
        " \n",
        "    d=((data1['part_cycle_time'].dt.minute)*60)+data2['part_cycle_time'].dt.second\n",
        "    df.insert(1, 'time_in_second', d)\n",
        "    df.insert(2,'cumsum_in_minute',((df['time_in_second'].cumsum())/60).astype(int))\n",
        "    df.insert(3,'cumsum_in_hour',((df['time_in_second'].cumsum())/3600).astype(int))\n",
        "    df.insert(1,'time_in_minute',data2['part_cycle_time'].dt.minute)\n",
        "    df.insert(0,'part_cycle_time_with_date',data3['part_cycle_time'])\n",
        "    df['material'].replace({'MILD_STEEL':1},inplace=True)\n",
        "    df['quality1']=df['quality']\n",
        "    df.drop(\"quality\",axis=1,inplace=True)\n",
        "    df.rename(columns={\"quality1\":\"quality\"},inplace=True)\n",
        "    df['running_tool_number'].fillna(0, inplace=True)\n",
        "    df['tool_changed'].fillna(0,inplace=True)\n",
        "    #self.knnImpute(df,'tolerance')\n",
        "    df['tolerance'].fillna(df['tolerance'].mean(),inplace=True) \n",
        "    df['cutting_speed'].fillna(df['cutting_speed'].mean(),inplace=True)\n",
        "    #df['cutting_speed'].fillna(df['cutting_speed'].median(),inplace=True)\n",
        "    # for i in range(len(l1)):\n",
        "    #   comparison=l1[i]==l2[i]\n",
        "    #   if (comparison == False):\n",
        "    #     print(f\"{l1[i]} and {l2[i]}\")\n",
        "    #     break\n",
        "    return df\n",
        "\n",
        "\n",
        "### FOR MAKING A NEW DATA AFTER DROPPING SOME OF THE COLUMNS ###\n",
        "\n",
        "  def newData(self):\n",
        "    df=self.dataManipulation()\n",
        "    df_new=df\n",
        "    df_new.drop(columns=['part_cycle_time','time_in_minute','time_in_second','cumsum_in_minute','part_ID','program_number'],inplace=True)\n",
        "    df_new.drop(columns=['part_cycle_time_with_date','part_count','running_tool_number','tool_changed','material'],inplace=True)\n",
        "    return df_new\n",
        "\n",
        "  # def normalizedData(self):\n",
        "  #   df=self.newData()\n",
        "  #   d_var=df.drop(columns=['cumsum_in_hour','quality'])\n",
        "  #   normalized_df=((d_var-d_var.min())/(d_var.max()-d_var.min()))\n",
        "  #   return normalized_df\n",
        "\n",
        "### TO NORMALIZE THE DATA USING MINMAX NORMALIZER FROM SKLEARN LIBRARY ###\n",
        "## USE AS DEFAULT \n",
        "### this does not have 'cumsum_in_hour' as the index\n",
        "\n",
        "############# FEATURE SCALING #################################\n",
        "###############################################################\n",
        " \n",
        "  def normalizedData1(self):\n",
        "    df=self.newData()\n",
        "    #df.drop(columns=['cutting_speed'],inplace=True)\n",
        "    minmax = MinMaxScaler()\n",
        "    for x in df.columns:\n",
        "      if (x!='cumsum_in_hour' and x!='quality'):\n",
        "        df[x] = minmax.fit_transform(np.array(df[x]).reshape(-1,1))\n",
        "    #df.style.background_gradient(cmap='Blues')\n",
        "    #df.set_index('cumsum_in_hour',inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "### TO NORMALIZE THE DATA USING ROBUST SCALER FROM SKLEARN LIBRARY ###\n",
        "### USE WHEN DATA HAS OUTLIERS\n",
        "\n",
        "  def normalizedData2(self):\n",
        "    df=self.newData()\n",
        "    #df.drop(columns=['cutting_speed'],inplace=True)\n",
        "    robustscaler=RobustScaler()\n",
        "    for x in df.columns:\n",
        "      if (x!='cumsum_in_hour' and x!='quality'):\n",
        "        df[x] = robustscaler.fit_transform(np.array(df[x]).reshape(-1,1))\n",
        "    #df.style.background_gradient(cmap='Blues')\n",
        "    #df.set_index('cumsum_in_hour',inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "  def normalizedData3(self):\n",
        "    df=self.newData()\n",
        "    standard_scaler=StandardScaler()\n",
        "    for x in df.columns:\n",
        "      if (x!='cumsum_in_hour' and x!='quality'):\n",
        "        df[x] = standard_scaler.fit_transform(np.array(df[x]).reshape(-1,1))\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## cutting speed, depth of cut, diameter, and tolerance were removed as features\n",
        "## as they had outliers greater than 45%\n",
        "  def normalizedData(self):\n",
        "    df=self.normalizedData2()\n",
        "    df.set_index('cumsum_in_hour',inplace=True)\n",
        "    df.drop(columns=['cutting_speed','depth_of_cut','diameter','tolerance','flow_rate','noise','spindle_speed'],inplace=True)\n",
        "    return df\n",
        "\n",
        "  def dataDivPart(self):\n",
        "    df=self.normalizedData1()\n",
        "    #df.set_index('cumsum_in_hour',inplace=True)\n",
        "    #df.drop(columns=['cutting_speed','depth_of_cut','diameter','tolerance','flow_rate','noise','spindle_speed'],inplace=True)\n",
        "    np.random.seed(1)\n",
        "    totalItem=1000\n",
        "    randomNum1=list(np.random.randint(60,1800,totalItem))\n",
        "    #randomList=list(sum(randomNum1[0:x+1] for x in range (len(randomNum1))))\n",
        "    randomList=[sum(randomNum1[0:x+1]) for x in range(0,len(randomNum1))]\n",
        "    #print(randomNum1)\n",
        "    df_div=[]\n",
        "    length=[]\n",
        "    index=0\n",
        "    for i,val in enumerate(randomList):\n",
        "      df_div.append(df.iloc[index:val,:])\n",
        "      index=val \n",
        "      length.append(len(df_div[-1]))\n",
        "    #print(length)\n",
        "    freq = {} \n",
        "    count_freq=[]\n",
        "    for items in length: \n",
        "      freq[items] = length.count(items)\n",
        "      count_freq.append(freq[items])\n",
        "\n",
        "    for key, value in freq.items(): \n",
        "      #print (\"% d : % d\"%(key, value))\n",
        "      if (value==(np.max(count_freq))):\n",
        "        #print(key)\n",
        "        break\n",
        "\n",
        "    # print(np.max(count_freq))\n",
        "    # print(df_div[0:6])\n",
        "    # print(\"Max is\")\n",
        "    # print(np.max(length))\n",
        "    # print(\"Min is\")\n",
        "    # print(np.min(length))\n",
        "    # print(\"Mean is\")\n",
        "    # print(np.mean(length))\n",
        "    # print(\"Median is\")\n",
        "    # print(np.median(length))\n",
        "\n",
        "\n",
        "    return df_div\n",
        "\n",
        "\n",
        "\n",
        "### this had the part_cycle_time column extra as compared to the previous method\n",
        "  def dataDivPartNew(self):\n",
        "    df=self.normalizedData()\n",
        "    df1=self.loadData()\n",
        "    df.insert(0,'part_cycle_time',df1['part_cycle_time'])\n",
        "    np.random.seed(1)\n",
        "    totalItem=1000\n",
        "    randomNum1=list(np.random.randint(60,1800,totalItem))\n",
        "    #randomList=list(sum(randomNum1[0:x+1] for x in range (len(randomNum1))))\n",
        "    randomList=[sum(randomNum1[0:x+1]) for x in range(0,len(randomNum1))]\n",
        "    df_div_new=[]\n",
        "    index=0\n",
        "    for i,val in enumerate(randomList):\n",
        "      df_div_new.append(df.iloc[index:val,:])\n",
        "      index=val \n",
        "    return df_div_new\n",
        "\n",
        "##### for the dataset on which the model is to be trained #####  \n",
        "\n",
        "  def newTrainTestVal(self):\n",
        "    df=self.dataDivPart()\n",
        "    #print(len(df))\n",
        "    spindle_load=[]\n",
        "    current=[]\n",
        "    spindle_temp=[]\n",
        "    feed=[]\n",
        "    vibration_x=[]\n",
        "    vibration_z=[]\n",
        "    vibration_spindle=[]\n",
        "    cutting_speed=[]\n",
        "    depth_of_cut=[]\n",
        "    diameter=[]\n",
        "    tolerance=[]\n",
        "    flow_rate=[]\n",
        "    noise=[]\n",
        "    spindle_speed=[]\n",
        "    quality=[]\n",
        "    quality1=[]\n",
        "    #'cutting_speed','depth_of_cut','diameter','tolerance','flow_rate','noise','spindle_speed'\n",
        "\n",
        "    for i in range(len(df)):\n",
        "      df[i]['quality'].fillna(0,inplace=True)\n",
        "      df[i].drop(columns=['cumsum_in_hour'],inplace=True)\n",
        "      spindle_load.append(np.mean(df[i]['spindle_load_%']))\n",
        "      current.append(np.mean(df[i]['current']))\n",
        "      spindle_temp.append(np.mean(df[i]['spindle_temp']))\n",
        "      feed.append(np.mean(df[i]['feed']))\n",
        "      vibration_x.append(np.mean(df[i]['vibration_X']))\n",
        "      vibration_z.append(np.mean(df[i]['vibration_Z']))\n",
        "      vibration_spindle.append(np.mean(df[i]['vibration_spindle']))\n",
        "      # cutting_speed.append(np.mean(df[i]['cutting_speed']))\n",
        "      # depth_of_cut.append(np.mean(df[i]['depth_of_cut']))\n",
        "      # diameter.append(np.mean(df[i]['diameter']))\n",
        "      # tolerance.append(np.mean(df[i]['tolerance']))\n",
        "      # flow_rate.append(np.mean(df[i]['flow_rate']))\n",
        "      # noise.append(np.mean(df[i]['noise']))\n",
        "      # spindle_speed.append(np.mean(df[i]['spindle_speed']))\n",
        "      quality.append(df[i]['quality'].values.tolist())\n",
        "    for elem in quality:\n",
        "      for item in elem:\n",
        "        if (item!=0):\n",
        "          quality1.append(item)\n",
        "    df_new=pd.DataFrame(columns=['spindle_load_%','current','spindle_temp','feed','vibration_X','vibration_Z','vibration_spindle','quality'])\n",
        "    df_new['spindle_load_%']=spindle_load\n",
        "    df_new['current']=current\n",
        "    df_new['spindle_temp']=spindle_temp\n",
        "    df_new['feed']=feed\n",
        "    df_new['vibration_X']=vibration_x\n",
        "    df_new['vibration_Z']=vibration_z\n",
        "    df_new['vibration_spindle']=vibration_spindle\n",
        "    # df_new['cutting_speed']=cutting_speed\n",
        "    # df_new['depth_of_cut']=depth_of_cut\n",
        "    # df_new['tolerance']=tolerance\n",
        "    # df_new['diameter']=diameter\n",
        "    # df_new['flow_rate']=flow_rate\n",
        "    # df_new['noise']=noise\n",
        "    # df_new['spindle_speed']=spindle_speed\n",
        "    df_new['quality']=quality1\n",
        "    \n",
        "    #print(df_new)\n",
        "    return df_new\n",
        "   \n",
        "\n",
        "\n",
        "### TO GET SOME INFORMATION ABOUT THE VARIOUS COLUMNS IN THE DATA ###\n",
        "\n",
        "  def dataExploration(self):\n",
        "    ## To get the information about the various columns in the dataframe\n",
        "    df=self.dataDivPartNew()[0]\n",
        "    # print(df.info())\n",
        "    # print(\"\\n\")\n",
        "    # print(df.describe())\n",
        "    # print(\"\\n\")\n",
        "    print(df.columns)\n",
        "    # print(\"\\n\")\n",
        "    # print(df.shape)\n",
        "    # print(\"\\n\")\n",
        "    # print(df.describe().shape)\n",
        "    # print(\"\\n\")\n",
        "    # #df['part_ID'].value_counts()\n",
        "    # print(df.isnull().sum())\n",
        "    # print(\"\\n\")\n",
        "    return df\n",
        "\n",
        "\n",
        "### TO MAKE THE LINEPLOT ###\n",
        "\n",
        "  def lineplot(self):\n",
        "    df=self.newTrainTestVal()\n",
        "    #df.set_index('cumsum_in_hour',inplace=True) \n",
        "    df.plot(subplots=True,linewidth=2.0,figsize=(30,30))\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.figure()\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### TO MAKE THE BOXPLOT ###\n",
        "\n",
        "  def boxgraph(self):\n",
        "    df=self.newTrainTestVal()\n",
        "    #df.boxplot(figsize=(15,15),grid=True,by='quality')\n",
        "    df.boxplot(figsize=(15,15),grid=True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#### Observations from a box plot \n",
        "### Outliers are data points observed outside the whisker at a certain range   [Q1-1.5*IQR and Q3+1.5*IQR] IQR-> Inter-Quartile Range    (observed in spindle speed, noise, vibration_z and cutting speed)\n",
        "### If the 50 percentile line is not in the middle then the data does not come from a symmetric distributrion\n",
        "### 50 percentile line is closer to the Q1 line it is right-skewed as is oberved in many cases\n",
        "### more the length more the inter-quartile range more is the spread of the distribution\n",
        "\n",
        "\n",
        "  def countOutlier(self):\n",
        "    df=self.newTrainTestVal()\n",
        "    df.drop(columns=['quality'],inplace=True)\n",
        "    pos=[]\n",
        "    count=0\n",
        "    for i in df.columns:\n",
        "      x=df.loc[:,i]\n",
        "      iqr=np.subtract(*np.percentile(x, [75, 25]))\n",
        "      q1=np.percentile(x,25)\n",
        "      q3=np.percentile(x,75)\n",
        "      o1=q1-(1.5*iqr)\n",
        "      o2=q3+(1.5*iqr)\n",
        "      for j in range(len(df)):\n",
        "        if ((x[j]<o1) | (x[j]>o2)):\n",
        "          pos.append(True)   ## outliers would be assigned a value true\n",
        "          count=count+1\n",
        "        else:\n",
        "          pos.append(False)\n",
        "      print(f\"The column {i} has {(count/len(df))*100}% outlier\")\n",
        "    \n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### TO MAKE THE HISTOGRAM ###\n",
        "  def histogram(self):\n",
        "    df=self.newTrainTestVal()\n",
        "    plt.figure(figsize=(20,20))\n",
        "    #df.set_index('cumsum_in_hour',inplace=True)\n",
        "    df.hist(grid=True,bins=15,color='steelblue', edgecolor='black', linewidth=2.0)\n",
        "    #plt.tight_layout(rect=(0, 0, 1.2, 1.2)) \n",
        "    # plt.tick_params(axis='x',labelsize=10)\n",
        "    # plt.tick_params(axis='y',labelsize=10)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "### TO GET THE PEARSON AND SPEARMAN RANK CORRELATION COEFFICIENT ###\n",
        "  def correlationCoefficient(self):\n",
        "    df=self.newTrainTestVal()\n",
        "    #df.set_index(\"cumsum_in_hour\",inplace=True)\n",
        "    corr=df.corr(method='pearson',min_periods=1)\n",
        "    corr1=df.corr(method='spearman',min_periods=1)\n",
        "    return corr,corr1\n",
        "\n",
        "\n",
        "# Correlation Matrix Heatmap\n",
        "  def correlationHeatMap(self):\n",
        "    df=self.newTrainTestVal()\n",
        "    #df.set_index(\"cumsum_in_hour\",inplace=True)\n",
        "    f, ax = plt.subplots(figsize=(10, 6))\n",
        "    corr = df.corr(method=\"pearson\")\n",
        "    sns.heatmap(round(corr,2), annot=True, ax=ax, cmap=\"coolwarm\",fmt='.2f',\n",
        "                    linewidths=.05)\n",
        "    f.subplots_adjust(top=0.93)\n",
        "    f.suptitle('Pearson Correlation Heatmap', fontsize=14)\n",
        "\n",
        "\n",
        "### TO GET THE HEAT MAP ###\n",
        "\n",
        "  def heatmap(self):\n",
        "    df=self.newTrainTestVal()\n",
        "    #df.set_index(\"cumsum_in_hour\",inplace=True)\n",
        "    df.drop(columns=['quality'],inplace=True)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    sns.heatmap(df,cmap='RdYlGn_r')\n",
        "    #plt.pcolor(df)\n",
        "    plt.show()\n",
        "\n",
        "### TO GET THE DISTRIBUTION PLOT ###\n",
        "  def distplot(self,x):\n",
        "    df=self.newTrainTestVal()\n",
        "    sns.distplot(df[x])\n",
        "    #sns.kdeplot(df[x], shade=True, color='steelblue')\n",
        "    \n",
        "\n",
        "# Pair-wise Scatter Plots\n",
        "  def pairwiseScatter(self):\n",
        "    df=self.newTrainTestVal()\n",
        "    \n",
        "    cols = ['spindle_load_%', 'current',\n",
        "        'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "        'vibration_spindle']\n",
        "    #f, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.pairplot(df[cols], height=1.8, aspect=1.8,\n",
        "                      plot_kws=dict(edgecolor=\"k\", linewidth=0.5),\n",
        "                      diag_kind=\"kde\", diag_kws=dict(shade=True))\n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# https://adtk.readthedocs.io/en/stable/quickstart.html\n",
        "  def adtk(self,i):\n",
        "    df=self.dataDivPartNew()\n",
        "    df[i].drop(columns=['cumsum_in_hour','quality'],inplace=True)\n",
        "    df[i].set_index(\"part_cycle_time\",inplace=True)\n",
        "    # series=validate_series(df[i])\n",
        "    # plot(series)\n",
        "\n",
        "    seasonal_ad = SeasonalAD()\n",
        "    anomalies = seasonal_ad.fit_detect(df[i])\n",
        "    plot(s_train, anomaly=anomalies, anomaly_color=\"red\", anomaly_tag=\"marker\")\n",
        "\n",
        "\n",
        "############## k-means algorithm ##################################\n",
        "###################################################################\n",
        "\n",
        "# k-means is a widely used clustering algorithm. \n",
        "# It creates ‘k’ similar clusters of data points. \n",
        "# Data instances that fall outside of these groups \n",
        "# could potentially be marked as anomalies. \n",
        "# Before k-means clustering, in order to determine the the optimal number of clusters\n",
        "# elbow method was used.\n",
        "# Reference: https://towardsdatascience.com/time-series-of-price-anomaly-detection-13586cd5ff46\n",
        "\n",
        "\n",
        "\n",
        "# Now to find out the number of components (features) to keep\n",
        "## is done using PCA for dimensionality reduction\n",
        "## Standard Scaler not used because the sns.distplot or the distribution plot did not show a normal distribution\n",
        "  def pca(self):\n",
        "    df=self.newTrainTestVal()\n",
        "    data = df[['spindle_load_%', 'current',\n",
        "        'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "        'vibration_spindle']]\n",
        "    X_std = data.values\n",
        "    mean_vec = np.mean(X_std, axis=0)\n",
        "    cov_mat = np.cov(X_std.T)\n",
        "    eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
        "    eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
        "    eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
        "    tot = sum(eig_vals)\n",
        "    var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
        "    cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(range(len(var_exp)), var_exp, alpha=0.3, align='center', label='individual explained variance', color = 'g')\n",
        "    plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')\n",
        "    plt.ylabel('Explained variance ratio')\n",
        "    plt.xlabel('Principal components')\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  def pca1(self):\n",
        "    df=self.newTrainTestVal()\n",
        "    # Take useful feature and standardize them\n",
        "    data = df[['spindle_load_%', 'current',\n",
        "        'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "        'vibration_spindle']]\n",
        "    X = data.values\n",
        "    #X_std = StandardScaler().fit_transform(X)\n",
        "    # reduce to 2 important features\n",
        "    pca = PCA(n_components=2)\n",
        "    data_new = pca.fit_transform(X)\n",
        "    # standardize these 2 new features\n",
        "    # scaler = StandardScaler()\n",
        "    # np_scaled = scaler.fit_transform(data)\n",
        "    # df['quality'].fillna(1,inplace=True)\n",
        "    # df['quality'].replace({'ACCEPTED':1,'REJECTED':0},inplace=True)\n",
        "    \n",
        "    pcaDataframe = pd.DataFrame(data_new,columns = ['principal_component_1', 'principal_component_2'])\n",
        "    pcaData=pcaDataframe\n",
        "    pcaDataframe['quality']=df['quality']\n",
        "    #print(pcaDataframe)\n",
        "    pcaDataframe['quality'].replace({'ACCEPTED':1,'REJECTED':0},inplace=True)\n",
        "##########################################################\n",
        "    # fig = plt.figure(figsize = (15,15))\n",
        "    # plt.scatter(data_new[:, 0], data_new[:, 1],\n",
        "    #         c=pcaDataframe.quality, edgecolor='none', alpha=0.5,\n",
        "    #         cmap=plt.cm.get_cmap('Blues', 10))\n",
        "    # #plt.axis([0,0.2,0,0.2])\n",
        "    # plt.xlabel('component 1')\n",
        "    # plt.ylabel('component 2')\n",
        "    # plt.colorbar()\n",
        "    # plt.show()\n",
        "#############################################\n",
        "    # fig = plt.figure(figsize = (15,15))\n",
        "    # ax = fig.add_subplot(1,1,1) \n",
        "    # # Setting X-axis and Y-axis limits\n",
        "    # # ax.set_xlim([0.0, 0.05])\n",
        "    # # ax.set_ylim([0, 0.05])\n",
        "    # ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "    # ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "    # ax.set_title('2 component PCA', fontsize = 20)\n",
        "    # targets = [1, 0]\n",
        "    # colors = ['r', 'b']\n",
        "    # for target, color in zip(targets,colors):\n",
        "    #   indicesToKeep = pcaDataframe['quality'] == target\n",
        "    #   ax.scatter(pcaDataframe.loc[indicesToKeep, 'principal_component_1']\n",
        "    #               , pcaDataframe.loc[indicesToKeep, 'principal_component_2']\n",
        "    #               , c = color\n",
        "    #               , s = 50)\n",
        "    # ax.legend(targets)\n",
        "    # ax.grid()\n",
        "    # plt.show()\n",
        "    return pcaData\n",
        "\n",
        " \n",
        "\n",
        "  def tsne(self):\n",
        "    df=self.newTrainTestVal()\n",
        "    data = df[['spindle_load_%', 'current',\n",
        "        'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "        'vibration_spindle']]\n",
        "    X = data.values\n",
        "    t_sne=TSNE(n_components=2)\n",
        "    X_embedded = t_sne.fit_transform(X)\n",
        "    X_embedded.shape\n",
        "    tsneDataframe=pd.DataFrame(X_embedded,columns=['component_1','component_2'])\n",
        "    tsneData=tsneDataframe\n",
        "    tsneDataframe['quality']=df['quality']\n",
        "    tsneDataframe['quality'].replace({'ACCEPTED':1,'REJECTED':0},inplace=True)\n",
        "    # print(tsneDataframe)\n",
        "    #tsneDataframe.plot.scatter(x='component_1',y='component_2')\n",
        "\n",
        "    # fig = plt.figure(figsize = (15,15))\n",
        "    # ax = fig.add_subplot(1,1,1) \n",
        "    # Setting X-axis and Y-axis limits\n",
        "    # ax.set_xlim([0.0, 0.05])\n",
        "    # ax.set_ylim([0, 0.05])\n",
        "    # ax.set_xlabel('Component 1', fontsize = 15)\n",
        "    # ax.set_ylabel('Component 2', fontsize = 15)\n",
        "    # ax.set_title('t-SNE Visualization', fontsize = 20)\n",
        "    # targets = [1, 0]\n",
        "    # colors = ['r', 'b']\n",
        "    # for target, color in zip(targets,colors):\n",
        "    #   indicesToKeep = tsneDataframe['quality'] == target\n",
        "    #   ax.scatter(tsneDataframe.loc[indicesToKeep, 'component_1']\n",
        "    #               , tsneDataframe.loc[indicesToKeep, 'component_2']\n",
        "    #               , c = color\n",
        "    #               , s = 50)\n",
        "    # ax.legend(targets)\n",
        "    # ax.grid()\n",
        "    # plt.show()\n",
        "    return tsneData\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # def elbowplot(self):\n",
        "  #   df=self.newTrainTestVal()\n",
        "  #   data = df[['spindle_load_%', 'current', \n",
        "  #      'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "  #      'vibration_spindle', 'flow_rate', \n",
        "  #      'depth_of_cut', 'diameter', 'tolerance']].values\n",
        "  #   n_cluster = range(1, 15)\n",
        "  #   kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\n",
        "  #   scores = [kmeans[i].score(data) for i in range(len(kmeans))]\n",
        "\n",
        "  #   fig, ax = plt.subplots(figsize=(10,6))\n",
        "  #   ax.plot(n_cluster, scores)\n",
        "  #   plt.xlabel('Number of Clusters')\n",
        "  #   plt.ylabel('Score')\n",
        "  #   plt.title('Elbow Curve')\n",
        "  #   plt.show()\n",
        "\n",
        "\n",
        "### this is the elbow plot to determine the optimal number of clusters \n",
        "### to be considered \n",
        "### Here the sum of squared distances (inertia) is plotted against\n",
        "### the number of clusters \n",
        "### the elbow point determines the optimal number of clusters\n",
        "  def elbowplot1(self):\n",
        "    df=self.newTrainTestVal()\n",
        "    data = df[['spindle_load_%', 'current',\n",
        "        'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "        'vibration_spindle']].values\n",
        "    Sum_of_squared_distances = []\n",
        "    K = range(1,15)\n",
        "    for k in K:\n",
        "      km = KMeans(n_clusters=k)\n",
        "      km = km.fit(data)\n",
        "      Sum_of_squared_distances.append(km.inertia_)\n",
        "    plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
        "    plt.xlabel('Number of clusters (k)')\n",
        "    plt.ylabel('Sum_of_squared_distances')\n",
        "    plt.title('Elbow Method For Optimal k')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "## silhouette score vary between -1 to +1 so closer it is to 1 more better it is\n",
        "## the peak value which is closer to one is the ideal cluster value\n",
        "\n",
        "\n",
        "  def silhouetteScore(self):\n",
        "    df=self.newTrainTestVal()\n",
        "    data = df[['spindle_load_%', 'current',\n",
        "        'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "        'vibration_spindle']].values\n",
        "    sil = []\n",
        "    kmax = 6\n",
        "    K=range(2,kmax+1)\n",
        "    # dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2\n",
        "    for k in K:\n",
        "      kmeans = KMeans(n_clusters = k).fit(data)\n",
        "      labels = kmeans.labels_\n",
        "      sil.append(silhouette_score(data, labels, metric = 'euclidean'))\n",
        "    print(sil)\n",
        "    print(\"\\n\")\n",
        "    plt.plot(K, sil, 'bx-')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('Silhouette score')\n",
        "    plt.title('Silhouette score method For Optimal k')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "## from the elbow plot 1 it can be seen that the plot levels off after 3 that is the elbow point is at 3 but from the silhouette score n_cluster=2\n",
        "## n_cluster represent the number of clusters as well as the number of centroids generated\n",
        "## here y_means will show which training example belong to which cluster\n",
        "\n",
        "### As the ground truth or the actual labels were not available so the metrics used were those which did \n",
        "### not require ground truth labels\n",
        "\n",
        "  def kmeans(self):\n",
        "    pca=self.pca1()\n",
        "    X=pca.values\n",
        "    km = KMeans(n_clusters=2)\n",
        "    km.fit(X)\n",
        "    y_means=km.predict(X)\n",
        "    labels = km.labels_\n",
        "    print(\"The labels of each point\")\n",
        "    #print(labels)\n",
        "    print(\"The value of y_means is \\n\")\n",
        "    #print(y_means)\n",
        "    print(\"Sum of squared distances of samples to their closest cluster center is\\n\")\n",
        "    print(km.inertia_)\n",
        "    print(\"The value of the coordinates of the cluster center\\n\")\n",
        "    print(km.cluster_centers_)\n",
        "    print(\"The number of iterations run is\\n\")\n",
        "    print(km.n_iter_)\n",
        "    print(km.get_params)\n",
        "    print(f\"The shape of X is {X.shape}\")\n",
        "    score_silhouette=metrics.silhouette_score(X, labels, metric='euclidean')\n",
        "    print(f\"The silhouette score is {score_silhouette}\")\n",
        "    score_calinski=metrics.calinski_harabasz_score(X, labels)\n",
        "    print(f\"The Calinski-Harabasz Index is {score_calinski}\")\n",
        "    score_davies=metrics.davies_bouldin_score(X, labels)\n",
        "    print(f\"The Davies_Bouldin Index is {score_davies}\")\n",
        "\n",
        "    #Plotting\n",
        "    plt.scatter(X[y_means==0, 0], X[y_means==0, 1], s=100,  c='red', label ='Cluster 1')\n",
        "    plt.scatter(X[y_means==1, 0], X[y_means==1, 1], s=100, c='blue', label ='Cluster 2')\n",
        "  \n",
        "    plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=300, c='yellow', label = 'Centroids')\n",
        "    plt.title('KMeans')\n",
        "    plt.show()\n",
        "    return score_silhouette,score_calinski,score_davies\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Mini-Batch K-Means clustering\n",
        "\n",
        "\n",
        "  def miniBatchKmeans(self):\n",
        "    pca=self.pca1()\n",
        "    X=pca.values\n",
        "    km = MiniBatchKMeans(n_clusters=2)\n",
        "    km.fit(X)\n",
        "    y_means=km.predict(X)\n",
        "    labels = km.labels_\n",
        "    #print(\"The labels of each point\")\n",
        "    #print(labels)\n",
        "    print(\"The value of y_means is \\n\")\n",
        "    #print(y_means)\n",
        "    print(\"Sum of squared distances of samples to their closest cluster center is\\n\")\n",
        "    print(km.inertia_)\n",
        "    print(\"The value of the coordinates of the cluster center\\n\")\n",
        "    print(km.cluster_centers_)\n",
        "    print(\"The number of iterations run is\\n\")\n",
        "    print(km.n_iter_)\n",
        "    print(km.get_params)\n",
        "    score_silhouette=metrics.silhouette_score(X, labels, metric='euclidean')\n",
        "    print(f\"The silhouette score is {score_silhouette}\")\n",
        "    score_calinski=metrics.calinski_harabasz_score(X, labels)\n",
        "    print(f\"The Calinski-Harabasz Index is {score_calinski}\")\n",
        "    score_davies=metrics.davies_bouldin_score(X, labels)\n",
        "    print(f\"The Davies_Bouldin Index is {score_davies}\")\n",
        "  \n",
        "    #Plotting\n",
        "    plt.scatter(X[y_means==0, 0], X[y_means==0, 1], s=100,  c='red', label ='Cluster 1')\n",
        "    plt.scatter(X[y_means==1, 0], X[y_means==1, 1], s=100, c='blue', label ='Cluster 2')\n",
        "    # plt.scatter(X[y_means==2, 0], X[y_means==2, 1], s=100,  c='green', label ='Cluster 3')\n",
        "    plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=300, c='yellow', label = 'Centroids')\n",
        "    plt.title('KMeans')\n",
        "    plt.show()\n",
        "    return score_silhouette,score_calinski,score_davies\n",
        "\n",
        "\n",
        "### DBScan gives better clustering result as compared to kmeans clustering\n",
        "## all samples are assigned only one cluster\n",
        "### noisy samples are given label -1\n",
        "\n",
        "  def dbscan(self):\n",
        "    pca=self.pca1()\n",
        "    X=pca.values\n",
        "    db_scan=DBSCAN()\n",
        "    labels=db_scan.fit_predict(X)\n",
        "    print(\"The labels are\\n\")\n",
        "    print(labels)\n",
        "    print(db_scan.get_params)\n",
        "    score_silhouette=metrics.silhouette_score(X, labels, metric='euclidean')\n",
        "    print(f\"The silhouette score is {score_silhouette}\")\n",
        "    score_calinski=metrics.calinski_harabasz_score(X, labels)\n",
        "    print(f\"The Calinski-Harabasz Index is {score_calinski}\")\n",
        "    score_davies=metrics.davies_bouldin_score(X, labels)\n",
        "    print(f\"The Davies_Bouldin Index is {score_davies}\")\n",
        "    plt.scatter(X[labels==0, 0], X[labels==0, 1], s=100,  c='red', label ='Cluster 1')\n",
        "    plt.scatter(X[labels==-1, 0], X[labels==-1, 1], s=100, c='blue', label ='Cluster 2')\n",
        "    plt.title('DBSCAN')\n",
        "    plt.show()\n",
        "    return score_silhouette,score_calinski,score_davies\n",
        "  \n",
        "\n",
        "### AFFINITY PROPAGATION ###########\n",
        "  def affinity(self):\n",
        "    pca=self.pca1()\n",
        "    X=pca.values\n",
        "    af = AffinityPropagation().fit(X)\n",
        "    cluster_centers_indices = af.cluster_centers_indices_\n",
        "    labels = af.labels_\n",
        "    print(\"The labels are\\n\")\n",
        "    print(labels)\n",
        "    print(cluster_centers_indices)\n",
        "    score_silhouette=metrics.silhouette_score(X, labels, metric='euclidean')\n",
        "    print(f\"The silhouette score is {score_silhouette}\")\n",
        "    score_calinski=metrics.calinski_harabasz_score(X, labels)\n",
        "    print(f\"The Calinski-Harabasz Index is {score_calinski}\")\n",
        "    score_davies=metrics.davies_bouldin_score(X, labels)\n",
        "    print(f\"The Davies_Bouldin Index is {score_davies}\")\n",
        "    return score_silhouette,score_calinski,score_davies\n",
        "\n",
        "##### GAUSSIAN MIXTURE AND BAYESIAN GAUSSIAN MIXTURE #####################\n",
        "\n",
        "  def gaussian(self):\n",
        "    pca=self.pca1()\n",
        "    X=pca.values\n",
        "    #gm=GaussianMixture(n_components=2)  ######### for gaussian mixture #########\n",
        "    gm=BayesianGaussianMixture(n_components=2)   ######## for bayesian gaussian mixture ############\n",
        "    gm.fit(X)\n",
        "    labels=gm.predict(X) \n",
        "    print(\"The labels are\\n\")\n",
        "    print(labels)\n",
        "    print(gm.predict_proba(X))\n",
        "    score_silhouette=metrics.silhouette_score(X, labels, metric='euclidean')\n",
        "    print(f\"The silhouette score is {score_silhouette}\")\n",
        "    score_calinski=metrics.calinski_harabasz_score(X, labels)\n",
        "    print(f\"The Calinski-Harabasz Index is {score_calinski}\")\n",
        "    score_davies=metrics.davies_bouldin_score(X, labels)\n",
        "    print(f\"The Davies_Bouldin Index is {score_davies}\")\n",
        "    plt.scatter(X[labels==0, 0], X[labels==0, 1], s=100,  c='red', label ='Cluster 1')\n",
        "    plt.scatter(X[labels==1, 0], X[labels==1, 1], s=100, c='blue', label ='Cluster 2')\n",
        "    plt.title('Gaussian Mixture')\n",
        "    plt.show()\n",
        "    return score_silhouette,score_calinski,score_davies\n",
        "  \n",
        "########## for agglomerative clustering ############\n",
        "\n",
        "  def agglomerative(self):\n",
        "    pca=self.pca1()\n",
        "    X=pca.values\n",
        "    ac=AgglomerativeClustering(n_clusters=2)  \n",
        "    ac.fit(X)\n",
        "    labels=ac.labels_\n",
        "    print(\"The labels are\\n\")\n",
        "    print(labels)\n",
        "    score_silhouette=metrics.silhouette_score(X, labels, metric='euclidean')\n",
        "    print(f\"The silhouette score is {score_silhouette}\")\n",
        "    score_calinski=metrics.calinski_harabasz_score(X, labels)\n",
        "    print(f\"The Calinski-Harabasz Index is {score_calinski}\")\n",
        "    score_davies=metrics.davies_bouldin_score(X, labels)\n",
        "    print(f\"The Davies_Bouldin Index is {score_davies}\")\n",
        "    plt.scatter(X[labels==0, 0], X[labels==0, 1], s=100,  c='red', label ='Cluster 1')\n",
        "    plt.scatter(X[labels==1, 0], X[labels==1, 1], s=100, c='blue', label ='Cluster 2')\n",
        "    plt.title('Agglomerative clustering')\n",
        "    plt.show()\n",
        "    return score_silhouette,score_calinski,score_davies\n",
        "\n",
        "  def resultClustering(self):\n",
        "    s1,c1,d1=self.kmeans()\n",
        "    s2,c2,d2=self.miniBatchKmeans()\n",
        "    s3,c3,d3=self.dbscan()\n",
        "    s4,c4,d4=self.affinity()\n",
        "    s5,c5,d5=self.gaussian()\n",
        "    s6,c6,d6=self.agglomerative()\n",
        "    name=['kmeans clustering','mini-batch kmeans clustering','DBSCAN','Affinity Propagation','Gaussian Mixture','Agglomerative Clustering']\n",
        "    score_silhouette=[s1,s2,s3,s4,s5,s6]\n",
        "    score_calinski=[c1,c2,c3,c4,c5,c6]\n",
        "    score_davies=[d1,d2,d3,d4,d5,d6]\n",
        "    df=pd.DataFrame(columns=['Name of the clustering metric','Silhouette score','Calinski Harabasz Index','Davies Bouldin Index'])\n",
        "    df['Name of the clustering metric']=name\n",
        "    df['Silhouette score']=score_silhouette\n",
        "    df['Calinski Harabasz Index']=score_calinski\n",
        "    df['Davies Bouldin Index']=score_davies\n",
        "    df.sort_values(by='Silhouette score', inplace=True)\n",
        "    print(df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# To get the count of the Accepted and Rejected parts\n",
        "  def count(self):\n",
        "    df=self.newTrainTestVal()\n",
        "    df['quality'].replace({\"ACCEPTED\":1,\"REJECTED\":0},inplace=True)\n",
        "    #print(df['quality'].isnull().sum())\n",
        "    neg, pos = np.bincount(df['quality'])\n",
        "    total = neg + pos\n",
        "    print('Examples:\\n    Total: {}\\n    Rejected: {} ({:.2f}% of total)\\n'.format(\n",
        "        total, neg, 100 * neg / total))\n",
        "\n",
        "\n",
        "# The whole dataframe was divided into train, test and validation set\n",
        "# The validataion set is used during the model fitting to evaluate the loss and any metrics. \n",
        "# The test set is completely unused during the training phase and is only used at the end to evaluate \n",
        "# how well the model is performing on unseen data.\n",
        "# This is especially important in case of imbalanced data where overfitting is a significant concern. \n",
        "\n",
        "\n",
        "  def plot_2d_space(self, X, y, label='Classes'):   \n",
        "    colors = ['#1F77B4', '#FF7F0E']\n",
        "    markers = ['o', 's']\n",
        "    for l, c, m in zip(np.unique(y), colors, markers):\n",
        "      print(X[y==l].shape)\n",
        "      plt.scatter(\n",
        "          X[y==l, 0],\n",
        "          X[y==l, 1],\n",
        "          c=c, label=l, marker=m\n",
        "      )\n",
        "    plt.title(label)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  def trainTestValSplit(self,flag):\n",
        "    df=self.newTrainTestVal()\n",
        "    df['quality'].replace({\"ACCEPTED\":1,\"REJECTED\":0},inplace=True)\n",
        "    # print(df)\n",
        "    features = list(df.columns.values)\n",
        "    features.remove('quality')\n",
        "    #print(features)\n",
        "    X = df[features].values\n",
        "    y = df['quality'].values\n",
        "    # Class count\n",
        "    X1=df[features]\n",
        "    y1=df['quality']\n",
        "    label='quality'\n",
        "    # print(X1.shape)\n",
        "    # print(y1.shape)\n",
        "    count_class_0, count_class_1 = df.quality.value_counts()\n",
        "\n",
        "    # Divide by class\n",
        "    df_class_0 = df[df['quality'] == 0]\n",
        "    df_class_1 = df[df['quality'] == 1]\n",
        "    # print(len(df_class_0))\n",
        "    # print(len(df_class_1))\n",
        " \n",
        "    # Upsample minority class\n",
        "    df_class_0_upsampled = resample(df_class_0, \n",
        "                                    replace=True,     # sample with replacement\n",
        "                                    n_samples=20,    # to match majority class\n",
        "                                    random_state=123) # reproducible results\n",
        "    \n",
        "    # Combine majority class with upsampled minority class\n",
        "    df_upsampled = pd.concat([df_class_1, df_class_0_upsampled])\n",
        "    df_upsampled.sort_index(inplace=True)\n",
        "    \n",
        "    # Display new class counts\n",
        "    # print(df_upsampled.quality.value_counts())\n",
        "\n",
        "    # Separate majority and minority classes\n",
        "    df_minority = df_upsampled[df_upsampled.quality==0]\n",
        "    df_majority = df_upsampled[df_upsampled.quality==1]\n",
        "    # print((df_majority))\n",
        "    # print((df_minority))\n",
        "    \n",
        "    # Downsample majority class\n",
        "    df_majority_downsampled = resample(df_majority, \n",
        "                                    replace=True,    # sample without replacement\n",
        "                                    n_samples=980,     # to match minority class\n",
        "                                    random_state=123) # reproducible results\n",
        "    \n",
        "    # Combine minority class with downsampled majority class\n",
        "    df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
        "    df_downsampled.sort_index(inplace=True)\n",
        "    #print(df_downsampled.head(50))\n",
        "    \n",
        "    # Display new class counts\n",
        "    # print(df_downsampled.quality.value_counts())\n",
        "    features1 = list(df_downsampled.columns.values)\n",
        "    features1.remove('quality')\n",
        "    #print(features)\n",
        "    X2 = df_downsampled[features1]\n",
        "    y2 = df_downsampled['quality']\n",
        "    # print(X2.shape)\n",
        "    # print(y2.shape)\n",
        "    # print(df_downsampled.shape)\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "# ##### Original dataset dimensionality reduction using pca and then visualization\n",
        "    # pca = PCA(n_components=2)\n",
        "    # X2 = pca.fit_transform(X2)\n",
        "    # print(\"Imbalance dataset\")\n",
        "    # print(X2.shape)\n",
        "    # print(y2.shape)\n",
        "    #self.plot_2d_space(X2, y2, 'Imbalanced dataset (2 PCA components)')\n",
        "\n",
        "#  ### Random undersampling using python library imbalance   \n",
        "    rus = RandomUnderSampler(return_indices=True)\n",
        "    X_rus, y_rus, id_rus = rus.fit_sample(X2, y2)\n",
        "    #print('Removed indexes:', id_rus)\n",
        "    # print(\"Random under-sampling\")\n",
        "    # print(X_rus.shape,y_rus.shape)\n",
        "    #self.plot_2d_space(X_rus, y_rus, 'Random under-sampling')\n",
        "\n",
        "# ###### Random over-sampling ##############\n",
        "\n",
        "    ros = RandomOverSampler(return_indices=True)\n",
        "    X_ros, y_ros, id_rus= ros.fit_sample(X2, y2)\n",
        "    #print(X_ros.shape[0] - X2.shape[0], 'new random picked points')\n",
        "    #print('Removed indexes:', id_rus)\n",
        "    # print(\"Random over-sampling\")\n",
        "    # print(X_ros.shape,y_ros.shape)\n",
        "    #self.plot_2d_space(X_ros, y_ros, 'Random over-sampling')\n",
        "\n",
        "# # ##### TOMEK links ################################\n",
        "    tl = TomekLinks(return_indices=True, ratio='auto')\n",
        "    X_tl, y_tl, id_tl = tl.fit_sample(X2, y2)\n",
        "\n",
        "    #print('Removed indexes:', id_tl)\n",
        "    # print(\"Tomek links under-sampling\")\n",
        "    # print(X_tl.shape,y_tl.shape)\n",
        "    #self.plot_2d_space(X_tl, y_tl, 'Tomek links under-sampling')\n",
        "\n",
        "# # ################## CLUSTER CENTROIDS #####################\n",
        "    cc = ClusterCentroids(ratio={0: 3})\n",
        "    X_cc, y_cc = cc.fit_sample(X2, y2)\n",
        "    # print(\"Cluster Centroids under-sampling\")\n",
        "    # print(X_cc.shape,y_cc.shape)\n",
        "\n",
        "    #self.plot_2d_space(X_cc, y_cc, 'Cluster Centroids under-sampling')\n",
        "\n",
        "# ################### SMOTE #######################################\n",
        "\n",
        "    smote = SMOTE(ratio=0.25)\n",
        "    X_sm, y_sm = smote.fit_sample(X2, y2)\n",
        "    # print(\"SMOTE over-sampling\")\n",
        "    # print(X_sm.shape,y_sm.shape)\n",
        "\n",
        "    #self.plot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')\n",
        "\n",
        "# ############# SMOTE + TOMEK LINKS ################################\n",
        "\n",
        "    smt = SMOTETomek(ratio=0.25)\n",
        "    X_smt, y_smt = smt.fit_sample(X2, y2)\n",
        "    # print(\"SMOTE + Tomek links\")\n",
        "    # print(X_smt.shape,y_smt.shape)\n",
        "\n",
        "    #self.plot_2d_space(X_smt, y_smt, 'SMOTE + Tomek links')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # print(count_class_0,count_class_1)\n",
        "    # print(df_class_0)\n",
        "    # print(df_class_1)\n",
        "\n",
        "    ###training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
        "    ### However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on \n",
        "    ### a particular random choice for the pair of (train, validation) sets.\n",
        "\n",
        "    # A solution to this problem is a procedure called cross-validation (CV for short). \n",
        "    ### A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. \n",
        "    ###In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches \n",
        "    ###are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
        "\n",
        "    # A model is trained using k-1 of the folds as training data;\n",
        "\n",
        "    # the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
        "\n",
        "    # The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. \n",
        "    ##This approach can be computationally expensive, but does not waste too much data \n",
        "    ##(as is the case when fixing an arbitrary validation set), which is a major advantage in problems \n",
        "    ##such as inverse inference where the number of samples is very small.\n",
        "    #print(y_smt)\n",
        "    # kf = KFold(n_splits=3,shuffle=True)\n",
        "    # for train, test in kf.split(X,y):\n",
        "    #   X_train1=X[train]\n",
        "    #   X_test1=X[test]\n",
        "    #   y_train1=y[train]\n",
        "    #   y_test1=y[test]\n",
        "\n",
        "\n",
        "    if (flag==0):\n",
        "      return X_smt, y_smt\n",
        "    else:\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X_smt, y_smt, test_size=0.2, random_state=1)\n",
        "      if (flag ==1):\n",
        "        # print(X_train)\n",
        "        # print(X_test) \n",
        "        # print(y_train)\n",
        "        # print(y_test)  \n",
        "        return X_train, y_train, X_test, y_test\n",
        "      else:\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
        "        print(type(X_val))    \n",
        "        # print(y_val)\n",
        "        return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "    \n",
        "\n",
        "## the input shape does not include the total number of rows because the model feeds one training example per batch so the batch size is not important\n",
        "  def dlModel(self):\n",
        "    ### call the method with any value other than 0 or 1\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = self.trainTestValSplit(5)\n",
        "    X, y = self.trainTestValSplit(0)\n",
        "    randomNum1=list(np.random.randint(60,1800,1000))\n",
        "    # print(X.shape)\n",
        "    # print(y.shape)\n",
        "    # X_train1 = []\n",
        "    # y_train1 = []\n",
        "    # for i in range(60, (X.shape[0]+60)):\n",
        "    #   X_train1.append(X[i-60:i, 0])\n",
        "    #   y_train1.append(y[i, 0])\n",
        "    # X_train1, y_train1 = np.array(X_train1), np.array(y_train1)\n",
        "\n",
        "    # X_train1 = np.reshape(X_train1, (X_train1.shape[0], X_train1.shape[1], 1))\n",
        "    \n",
        "#### to prepare the model #####\n",
        "\n",
        "    model = keras.Sequential()\n",
        "    model.add(Conv1D(filters=32,kernel_size=2,activation='relu',input_shape=(5,7)))\n",
        "    model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(layers.Dense(20,activation='relu'))\n",
        "    #model.add(Embedding(2500, 128, dropout = 0.2))\n",
        "    # model.add(keras.layers.LSTM(units=50, return_sequences= True))\n",
        "    # model.add(Flatten())\n",
        "    # #model.add(layers.Dropout(0.2))\n",
        "    # model.add(layers.Dense(10,activation='relu'))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(2,activation='softmax'))\n",
        "#     metric = [\n",
        "#       keras.metrics.TruePositives(name='tp'),\n",
        "#       keras.metrics.FalsePositives(name='fp'),\n",
        "#       keras.metrics.TrueNegatives(name='tn'),\n",
        "#       keras.metrics.FalseNegatives(name='fn'), \n",
        "#       keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "#       keras.metrics.Precision(name='precision'),\n",
        "#       keras.metrics.Recall(name='recall'),\n",
        "#       keras.metrics.AUC(name='auc'),\n",
        "# ]\n",
        "    model.compile(optimizer='adam',\n",
        "              loss=['mse','SparseCategoricalCrossentropy'],\n",
        "              metrics='accuracy')\n",
        "    \n",
        "    history= model.fit(X_train, y_train, epochs=10, batch_size=1,validation_data=(X_val, y_val))\n",
        "    prediction=model.evaluate(X_test,  y_test, verbose=2)\n",
        "    print(prediction)\n",
        "    \n",
        "    print(model.summary())\n",
        "\n",
        "    loss_train = history.history['loss']\n",
        "    loss_val = history.history['val_loss']\n",
        "    epochs = range(1,11)\n",
        "    plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "    plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "    plt.title('Training and Validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "  \n",
        "    loss_train1 = history.history['accuracy']\n",
        "    loss_val1 = history.history['val_accuracy']\n",
        "    epochs = range(1,11)\n",
        "    plt.plot(epochs, loss_train1, 'g', label='Training accuracy')\n",
        "    plt.plot(epochs, loss_val1, 'b', label='validation accuracy')\n",
        "    plt.title('Training and Validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  def clusteringMethods(self):\n",
        "    #### call trainTestValSplit method by passing an argument 1 only\n",
        "    X_train, y_train, X_test, y_test = self.trainTestValSplit(1)\n",
        "    X, y =self.trainTestValSplit(0)\n",
        "    # kf = KFold(n_splits=2,shuffle=True)\n",
        "    # for train, test in kf.split(X,y):\n",
        "    #   X_train1=X[train]\n",
        "    #   X_test1=X[test]\n",
        "    #   y_train1=y[train]\n",
        "    #   y_test1=y[test]\n",
        "    random_state = 12883823\n",
        "    rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\n",
        "    for train, test in rkf.split(X,y):\n",
        "      X_train1=X[train]\n",
        "      X_test1=X[test]\n",
        "      y_train1=y[train]\n",
        "      y_test1=y[test]\n",
        "\n",
        "\n",
        "\n",
        "      models = {\n",
        "          'Random Forest': RandomForestClassifier(),\n",
        "          'AdaBoost': AdaBoostClassifier(),\n",
        "          'Gradient Boosting': GradientBoostingClassifier(),\n",
        "          'SVM': SVC(probability=True),\n",
        "          'Decision Tree': DecisionTreeClassifier()\n",
        "      }\n",
        "\n",
        "      params = {\n",
        "          'Random Forest': { 'n_estimators': [16, 32] },\n",
        "          'AdaBoost':  { 'n_estimators': [16, 32] },\n",
        "          'Gradient Boosting': { 'n_estimators': [16, 32], 'learning_rate': [0.8, 1.0] },\n",
        "          'SVM': {'kernel': ['rbf','linear'], 'C': [1, 10], 'gamma': [0.001, 0.0001]},\n",
        "          'Decision Tree': {'criterion':['gini','entropy']}\n",
        "            \n",
        "          \n",
        "      }\n",
        "      result=[]\n",
        "      keys = models.keys()\n",
        "      for key in keys:\n",
        "        model = models[key]\n",
        "        param = params[key]\n",
        "      ##### for hyperparameter tuning by using GridSearchCV and RandomizedSearchCV ####\n",
        "        grid = GridSearchCV(model, param)\n",
        "        grid.fit(X_train1,y_train1)\n",
        "        prediction=grid.predict(X_test1)\n",
        "        scoring = ['precision_macro', 'recall_macro','f1_macro']\n",
        "        scores = cross_val_score(grid, X, y, cv=5, scoring='f1')\n",
        "        scores1= cross_validate(grid, X, y, scoring=scoring,\n",
        "                          cv=5, return_train_score=False)\n",
        "        result.append(\n",
        "              {\n",
        "                  'name': key,\n",
        "                  'grid': grid,\n",
        "                  'classifier': grid.best_estimator_,\n",
        "                  'best score': grid.best_score_,\n",
        "                  'best params': grid.best_params_,\n",
        "                  'cv': grid.cv,\n",
        "                  'prediction': prediction,\n",
        "                  'cv_score': scores.mean(),\n",
        "                  'cv_validate_score': scores1 ['test_f1_macro'],\n",
        "                  'probability': grid.predict_proba(X_test1),\n",
        "                  'accuracy_score': accuracy_score(y_test1,prediction),\n",
        "                  'average_precision_score': metrics.average_precision_score(y_test1,prediction),\n",
        "                  'balanced_accuracy_score': metrics.balanced_accuracy_score(y_test1,prediction),\n",
        "                  'classification report' : metrics.classification_report(y_test1,prediction),\n",
        "                  'cohen kappa score' : metrics.cohen_kappa_score(y_test1,prediction),\n",
        "                  'confusion matrix' : metrics.confusion_matrix(y_test1,prediction),\n",
        "                  'precision_score' : metrics.precision_score(y_test1,prediction),\n",
        "                  'recall_score' : metrics.recall_score(y_test1,prediction),\n",
        "                  'f1 score': metrics.f1_score(y_test1,prediction),\n",
        "                  'Area Under the Receiver Operating Characteristic Curve': metrics.roc_auc_score(y_test1,prediction),\n",
        "                  'Receiver operating characteristic (ROC)': metrics.roc_curve(y_test1,prediction),\n",
        "                  'Precision-recall-f1 score for each class': metrics.precision_recall_fscore_support(y_test1,prediction)\n",
        "\n",
        "              }\n",
        "          )\n",
        "\n",
        "      #print(result)\n",
        "      d=pd.DataFrame(result)\n",
        "      ### sort on the basis of cross-validation score scoring method being f1\n",
        "      d.sort_values(by='cv_score',inplace=True)\n",
        "      print(d)\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### MAIN FUNCTION ######\n",
        "\n",
        "def main():\n",
        "  a=AnomalyDetection()\n",
        "  #print(a.loadData())\n",
        "  # print(\"\\n\")\n",
        "  #print(a.dataManipulation())\n",
        "  # print(\"\\n\")\n",
        "  #print(a.newData().shape)\n",
        "  # print(\"The normalized data with MinMaxScaler\\n\")\n",
        "  # print(a.normalizedData1())\n",
        "  # print(\"The normalized data with RobustScaler\\n\")\n",
        "  # print(a.normalizedData2())\n",
        "  #print(a.normalizedData3())\n",
        "  #a.dataDivPart()\n",
        "  # print(a.dataDivPartNew())\n",
        "  #a.newTrainTestVal()\n",
        "  #a.dataExploration()\n",
        "  # print(\"\\n\")\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  #a.lineplot()\n",
        "  #print(\"\\n\")\n",
        "  #a.lineplot('spindle_speed','cumsum_in_hour',1500)\n",
        "  #a.boxgraph()\n",
        "  #a.countOutlier()\n",
        "  #print(\"\\n\")\n",
        "  #a.histogram()\n",
        " \n",
        "  # print(\"The pearson coefficient is:\")\n",
        "  # print(a.correlationCoefficient()[0])\n",
        "  # print(\"\\n\")\n",
        "  # print(\"The spearman coefficient is:\")\n",
        "  # print(a.correlationCoefficient()[1])\n",
        "  #a.correlationHeatMap()\n",
        "  # a.heatmap()\n",
        "  # cols=['spindle_load_%', 'current', \n",
        "  #         'spindle_temp', 'feed', 'vibration_X', 'vibration_Z',\n",
        "  #         'vibration_spindle']\n",
        "  # for i in cols:  \n",
        "  #   a.distplot(i)\n",
        "  #   plt.show()\n",
        "  #a.pairwiseScatter()\n",
        "  #a.adtk(700)\n",
        "  #a.pca()\n",
        "  #a.pca1()\n",
        "  #a.tsne()\n",
        "  #a.elbowplot()\n",
        "  #a.elbowplot1()\n",
        "  #a.silhouetteScore()\n",
        "  #a.kmeans()\n",
        "  # print(\"Mini Batch KMeans below\\n\")\n",
        "  #a.miniBatchKmeans()\n",
        "  #a.dbscan()\n",
        "  #a.affinity()\n",
        "  # a.gaussian()\n",
        "  #a.agglomerative()\n",
        "  #a.resultClustering()\n",
        "  #a.count()\n",
        "  #a.trainTestValSplit(0)\n",
        "  # print(\"\\n\")\n",
        " \n",
        "  #a.dlModel()\n",
        "  a.clusteringMethods()\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  main()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                name  ...           Precision-recall-f1 score for each class\n",
            "3                SVM  ...  ([0.6428571428571429, 0.9564270152505446], [0....\n",
            "1           AdaBoost  ...  ([0.9663865546218487, 0.9919028340080972], [0....\n",
            "2  Gradient Boosting  ...  ([0.943089430894309, 0.9938775510204082], [0.9...\n",
            "4      Decision Tree  ...  ([0.936, 0.9959016393442623], [0.9831932773109...\n",
            "0      Random Forest  ...  ([0.9830508474576272, 0.9939393939393939], [0....\n",
            "\n",
            "[5 rows x 22 columns]\n",
            "                name  ...           Precision-recall-f1 score for each class\n",
            "3                SVM  ...  ([0.625, 0.872093023255814], [0.47619047619047...\n",
            "1           AdaBoost  ...  ([0.991869918699187, 0.9918200408997955], [0.9...\n",
            "2  Gradient Boosting  ...  ([0.9758064516129032, 0.9897540983606558], [0....\n",
            "0      Random Forest  ...  ([0.984, 0.9938398357289527], [0.9761904761904...\n",
            "4      Decision Tree  ...  ([0.9689922480620154, 0.9979296066252588], [0....\n",
            "\n",
            "[5 rows x 22 columns]\n",
            "                name  ...           Precision-recall-f1 score for each class\n",
            "3                SVM  ...  ([0.6554054054054054, 0.9505376344086022], [0....\n",
            "1           AdaBoost  ...  ([0.9827586206896551, 0.9879275653923542], [0....\n",
            "2  Gradient Boosting  ...  ([0.9663865546218487, 0.9898785425101214], [0....\n",
            "4      Decision Tree  ...  ([0.9836065573770492, 1.0], [1.0, 0.9959432048...\n",
            "0      Random Forest  ...  ([0.9830508474576272, 0.9919191919191919], [0....\n",
            "\n",
            "[5 rows x 22 columns]\n",
            "                name  ...           Precision-recall-f1 score for each class\n",
            "3                SVM  ...  ([0.5643564356435643, 0.8669275929549902], [0....\n",
            "1           AdaBoost  ...  ([0.952755905511811, 0.9917525773195877], [0.9...\n",
            "2  Gradient Boosting  ...  ([0.9389312977099237, 0.9958419958419958], [0....\n",
            "4      Decision Tree  ...  ([0.9538461538461539, 0.9979253112033195], [0....\n",
            "0      Random Forest  ...  ([0.975609756097561, 0.9897750511247444], [0.9...\n",
            "\n",
            "[5 rows x 22 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJgzjU6d3xn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class MongoDB(object):\n",
        "#   def __init__(self, dBName=None, collectionName=None):\n",
        "#     self.dBName=dBName\n",
        "#     self.collectionName= collectionName\n",
        "#     self.client=MongoClient(\"localhost\", 27017)\n",
        "#     self.DB=self.client[self.dBName]\n",
        "#     self.collection=self.DB[self.collectionName]\n",
        "\n",
        "#   def InsertData(self,path=None):\n",
        "\n",
        "#     copied_path=(\"Data_final_version.csv\")\n",
        "#     df=pd.read_csv(copied_path)\n",
        "#     data=df.to_dict('records')\n",
        "\n",
        "#     self.collection.insert_many(data,ordered=False)\n",
        "#     print(\"All the data has been exported to mongoDB server\")\n",
        "\n",
        "# if __name__==\"__main__\":\n",
        "#   mongodb=MongoDB(dBName='DataIntern',collectionName='AnomalyDetect')\n",
        "#   mongodb.InsertData(path=\"Data_final_version.csv\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBh5Mk0E5LQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "dba66899-6cdf-46df-dca4-e1e6f18fc68d"
      },
      "source": [
        "# import functools\n",
        "# import operator\n",
        "# l=[[[1,2,3],[4,5,6],[7,8,9]],[[10,11,12],[13,14,15],[16,17,18]],[[19,20,21],[22,23,24],[25,26,27]]]   # list to be flattened\n",
        "\n",
        "# output=[]\n",
        "# for i in range(len(l)):\n",
        "#   List_flat = functools.reduce(operator.iconcat, l[i],[])\n",
        "#   output.append(List_flat)\n",
        "\n",
        "# print(\"Original List:\",l)\n",
        "# print(\"Flattened List:\",output)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original List: [[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24], [25, 26, 27]]]\n",
            "Flattened List: [[1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18], [19, 20, 21, 22, 23, 24, 25, 26, 27]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwzfue6GRZ_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2b223d3d-0746-4a7d-ad7e-1ffec5905283"
      },
      "source": [
        "# import itertools\n",
        "# l=[[[1,2,3],[4,5,6],[7,8,9]],[[10,11,12],[13,14,15],[16,17,18]],[[19,20,21],[22,23,24],[25,26,27]]]\n",
        "\n",
        "# output=[]\n",
        "# for i in range(len(l)):\n",
        "#   List_flat = list(itertools.chain(*l[i]))\n",
        "#   output.append(List_flat)\n",
        "\n",
        "\n",
        "# print(\"Original List:\",l)\n",
        "# print(\"Flattened List:\",output)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original List: [[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24], [25, 26, 27]]]\n",
            "Flattened List: [[1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18], [19, 20, 21, 22, 23, 24, 25, 26, 27]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t83bHywsS8LL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}